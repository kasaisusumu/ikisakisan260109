from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, field_validator
from typing import Optional, List, Any, Dict, Union
import os
import json
import urllib.parse
import asyncio
import httpx 
from dotenv import load_dotenv
from openai import AsyncOpenAI 
import math
import re 
import traceback
from datetime import date, timedelta 
import random 
from contextlib import asynccontextmanager
import sqlite3
import hashlib

load_dotenv()

# ==========================================
# ğŸ”‘ è¨­å®š
# ==========================================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MAPBOX_ACCESS_TOKEN = os.getenv("MAPBOX_ACCESS_TOKEN")
GEOAPIFY_API_KEY = os.getenv("GEOAPIFY_API_KEY")
RAKUTEN_APP_ID = os.getenv("RAKUTEN_APP_ID")

# HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
http_client = None

# ==========================================
# ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ  (SQLite)
# ==========================================
DB_PATH = "cache.db"

def init_db():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS api_cache (
                key TEXT PRIMARY KEY,
                value TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        conn.commit()

def get_cache(key: str) -> Optional[Dict]:
    try:
        with sqlite3.connect(DB_PATH) as conn:
            cursor = conn.execute("SELECT value FROM api_cache WHERE key = ?", (key,))
            row = cursor.fetchone()
            if row:
                return json.loads(row[0])
    except Exception as e:
        print(f"Cache Read Error: {e}")
    return None

def set_cache(key: str, data: Any):
    try:
        with sqlite3.connect(DB_PATH) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO api_cache (key, value) VALUES (?, ?)",
                (key, json.dumps(data))
            )
            conn.commit()
    except Exception as e:
        print(f"Cache Write Error: {e}")

# ==========================================
# ğŸš€ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«
# ==========================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    init_db()
    global http_client
    http_client = httpx.AsyncClient(verify=False, timeout=30.0)
    print("âœ… System initialized with Strict Address Logic (No Gun, City Priority)")
    yield
    if http_client:
        await http_client.aclose()

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

aclient = AsyncOpenAI(
    api_key=OPENAI_API_KEY,
    max_retries=5,     
    timeout=60.0       
)

# --- å‹å®šç¾© ---
class ExistingSpot(BaseModel):
    name: str
    coordinates: Optional[List[float]] = None

class SearchRequest(BaseModel):
    query: str
    area: str = "" 

class VacantSearchRequest(BaseModel):
    latitude: float
    longitude: float
    radius: float = 3.0
    min_price: Optional[int] = None
    max_price: Optional[int] = None
    squeeze: List[str] = [] 
    checkin_date: Optional[str] = None
    checkout_date: Optional[str] = None
    adult_num: int = 2
    meal_type: Optional[str] = None
    polygon: Optional[List[List[float]]] = None 

class ImportRequest(BaseModel):
    url: str

class SuggestRequest(BaseModel):
    theme: str             
    existing_spots: List[Union[ExistingSpot, str, Dict[str, Any]]] = [] 
    liked_spots: list[str] = []
    noped_spots: list[str] = []
    area: str = ""         
    verify: bool = False   

class Spot(BaseModel):
    name: str
    description: str = ""
    coordinates: List[float]
    votes: int = 0
    stay_time: int = 0
    image_url: Optional[str] = None
    price: Optional[int] = None
    rating: Optional[float] = None
    url: Optional[str] = None
    source: str = "ai" 
    is_jalan: bool = False 
    mapbox_id: Optional[str] = None
    place_formatted: Optional[str] = None
    is_hotel: bool = False
    plan_id: Optional[str] = None
    room_class: Optional[str] = None
    status: str = "candidate"
    day: int = 0
    comment: str = ""
    link: str = ""
    category: str = "è¦³å…‰ã‚¹ãƒãƒƒãƒˆ"

    @field_validator('stay_time', 'votes', mode='before')
    def parse_int_fields(cls, v):
        if v is None: return 0
        if isinstance(v, (str, float)):
            try: return int(float(v))
            except: return 0
        return int(v)

    @field_validator('coordinates', mode='before')
    def parse_coordinates(cls, v):
        if isinstance(v, list):
            try: return [float(x) for x in v]
            except: return [0.0, 0.0]
        return v

class VerifyRequest(BaseModel):
    spots: List[Spot]

class OptimizeRequest(BaseModel):
    spots: List[Spot]
    start_time: str = "09:00" 
    end_time: str = "18:00"
    start_spot_name: Optional[str] = None
    end_spot_name: Optional[str] = None

class NearbyRequest(BaseModel):
    latitude: float
    longitude: float
    radius: int = 3000
    limit: int = 20
    mode: str = "standard" 

# ---------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ & ä½æ‰€æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯ (å¼·åŒ–ç‰ˆ)
# ---------------------------------------------------------

WIKI_HEADERS = {
    "User-Agent": "RouteHackerBot/1.0 (contact@example.com)"
}

def haversine_distance(coord1, coord2):
    R = 6371
    if not coord1 or not coord2: return float('inf')
    try:
        lat1, lon1 = math.radians(coord1[1]), math.radians(coord1[0])
        lat2, lon2 = math.radians(coord2[1]), math.radians(coord2[0])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
        return R * c
    except:
        return float('inf')

def is_inside_polygon(lat, lng, poly_coords):
    inside = False
    j = len(poly_coords) - 1
    for i in range(len(poly_coords)):
        xi, yi = poly_coords[i][0], poly_coords[i][1]
        xj, yj = poly_coords[j][0], poly_coords[j][1]
        
        intersect = ((yi > lat) != (yj > lat)) and \
            (lng < (xj - xi) * (lat - yi) / (yj - yi) + xi)
        if intersect:
            inside = not inside
        j = i
    return inside

PREF_NORMALIZER = {
    "åŒ—æµ·é“": "åŒ—æµ·é“", "Hokkaido": "åŒ—æµ·é“",
    "é’æ£®": "é’æ£®çœŒ", "Aomori": "é’æ£®çœŒ", "å²©æ‰‹": "å²©æ‰‹çœŒ", "Iwate": "å²©æ‰‹çœŒ",
    "å®®åŸ": "å®®åŸçœŒ", "Miyagi": "å®®åŸçœŒ", "ç§‹ç”°": "ç§‹ç”°çœŒ", "Akita": "ç§‹ç”°çœŒ",
    "å±±å½¢": "å±±å½¢çœŒ", "Yamagata": "å±±å½¢çœŒ", "ç¦å³¶": "ç¦å³¶çœŒ", "Fukushima": "ç¦å³¶çœŒ",
    "èŒ¨åŸ": "èŒ¨åŸçœŒ", "Ibaraki": "èŒ¨åŸçœŒ", "æ ƒæœ¨": "æ ƒæœ¨çœŒ", "Tochigi": "æ ƒæœ¨çœŒ",
    "ç¾¤é¦¬": "ç¾¤é¦¬çœŒ", "Gunma": "ç¾¤é¦¬çœŒ", "åŸ¼ç‰": "åŸ¼ç‰çœŒ", "Saitama": "åŸ¼ç‰çœŒ",
    "åƒè‘‰": "åƒè‘‰çœŒ", "Chiba": "åƒè‘‰çœŒ", "æ±äº¬": "æ±äº¬éƒ½", "Tokyo": "æ±äº¬éƒ½",
    "ç¥å¥ˆå·": "ç¥å¥ˆå·çœŒ", "Kanagawa": "ç¥å¥ˆå·çœŒ", "æ–°æ½Ÿ": "æ–°æ½ŸçœŒ", "Niigata": "æ–°æ½ŸçœŒ",
    "å¯Œå±±": "å¯Œå±±çœŒ", "Toyama": "å¯Œå±±çœŒ", "çŸ³å·": "çŸ³å·çœŒ", "Ishikawa": "çŸ³å·çœŒ",
    "ç¦äº•": "ç¦äº•çœŒ", "Fukui": "ç¦äº•çœŒ", "å±±æ¢¨": "å±±æ¢¨çœŒ", "Yamanashi": "å±±æ¢¨çœŒ",
    "é•·é‡": "é•·é‡çœŒ", "Nagano": "é•·é‡çœŒ", "å²é˜œ": "å²é˜œçœŒ", "Gifu": "å²é˜œçœŒ",
    "é™å²¡": "é™å²¡çœŒ", "Shizuoka": "é™å²¡çœŒ", "æ„›çŸ¥": "æ„›çŸ¥çœŒ", "Aichi": "æ„›çŸ¥çœŒ",
    "ä¸‰é‡": "ä¸‰é‡çœŒ", "Mie": "ä¸‰é‡çœŒ", "æ»‹è³€": "æ»‹è³€çœŒ", "Shiga": "æ»‹è³€çœŒ",
    "äº¬éƒ½": "äº¬éƒ½åºœ", "Kyoto": "äº¬éƒ½åºœ", "å¤§é˜ª": "å¤§é˜ªåºœ", "Osaka": "å¤§é˜ªåºœ",
    "å…µåº«": "å…µåº«çœŒ", "Hyogo": "å…µåº«çœŒ", "å¥ˆè‰¯": "å¥ˆè‰¯çœŒ", "Nara": "å¥ˆè‰¯çœŒ",
    "å’Œæ­Œå±±": "å’Œæ­Œå±±çœŒ", "Wakayama": "å’Œæ­Œå±±çœŒ", "é³¥å–": "é³¥å–çœŒ", "Tottori": "é³¥å–çœŒ",
    "å³¶æ ¹": "å³¶æ ¹çœŒ", "Shimane": "å³¶æ ¹çœŒ", "å²¡å±±": "å²¡å±±çœŒ", "Okayama": "å²¡å±±çœŒ",
    "åºƒå³¶": "åºƒå³¶çœŒ", "Hiroshima": "åºƒå³¶çœŒ", "å±±å£": "å±±å£çœŒ", "Yamaguchi": "å±±å£çœŒ",
    "å¾³å³¶": "å¾³å³¶çœŒ", "Tokushima": "å¾³å³¶çœŒ", "é¦™å·": "é¦™å·çœŒ", "Kagawa": "é¦™å·çœŒ",
    "æ„›åª›": "æ„›åª›çœŒ", "Ehime": "æ„›åª›çœŒ", "é«˜çŸ¥": "é«˜çŸ¥çœŒ", "Kochi": "é«˜çŸ¥çœŒ",
    "ç¦å²¡": "ç¦å²¡çœŒ", "Fukuoka": "ç¦å²¡çœŒ", "ä½è³€": "ä½è³€çœŒ", "Saga": "ä½è³€çœŒ",
    "é•·å´": "é•·å´çœŒ", "Nagasaki": "é•·å´çœŒ", "ç†Šæœ¬": "ç†Šæœ¬çœŒ", "Kumamoto": "ç†Šæœ¬çœŒ",
    "å¤§åˆ†": "å¤§åˆ†çœŒ", "Oita": "å¤§åˆ†çœŒ", "å®®å´": "å®®å´çœŒ", "Miyazaki": "å®®å´çœŒ",
    "é¹¿å…å³¶": "é¹¿å…å³¶çœŒ", "Kagoshima": "é¹¿å…å³¶çœŒ", "æ²–ç¸„": "æ²–ç¸„çœŒ", "Okinawa": "æ²–ç¸„çœŒ"
}

def extract_and_fix_address(raw_address: str) -> str:
    """
    è¤‡é›‘ãªæ–‡å­—åˆ—ã‹ã‚‰éƒ½é“åºœçœŒã¨å¸‚åŒºç”ºæ‘ã‚’ç‹¬ç«‹ã—ã¦æŠ½å‡ºã—ã€æ­£ã—ã„é †åºï¼ˆçœŒ+å¸‚ï¼‰ã§å†çµåˆã™ã‚‹ã€‚
    â˜…ãƒ«ãƒ¼ãƒ«: 
    1. ã€Œéƒ¡ã€ã¯å‰Šé™¤
    2. ã€Œå¸‚ã€ãŒã‚ã‚‹å ´åˆã¯åŒºç”ºæ‘ã‚’ç„¡è¦–ï¼ˆå¸‚ã‚’å„ªå…ˆï¼‰
    3. ã€Œç§‘å­¦åšç‰©é¤¨å¯Œå±±å¸‚ã€ã®ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ã¯æœ«å°¾ã®ã€Œå¯Œå±±å¸‚ã€ã‚’æ¡ç”¨
    """
    if not raw_address: return ""

    # 1. ã‚´ãƒŸæƒé™¤
    working_text = re.sub(r'(Japan|æ—¥æœ¬|ã€’\d{3}-\d{4})', ' ', raw_address)
    working_text = re.sub(r'[ \t,]+', ' ', working_text).strip()

    # â˜… éƒ¡ã‚’å‰Šé™¤ (ä¾‹: "æ„›çŸ¥éƒ¡æ±éƒ·ç”º" -> "æ±éƒ·ç”º", "çŸ³å·éƒ¡é‡ã€…å¸‚ç”º" -> "é‡ã€…å¸‚ç”º")
    working_text = re.sub(r'[ä¸€-é¾ ã-ã‚“ã‚¡-ãƒ³]{1,6}éƒ¡', '', working_text)

    # 2. éƒ½é“åºœçœŒã®ç‰¹å®šã¨æŠ½å‡º
    found_pref = ""
    for k, v in PREF_NORMALIZER.items():
        if k in working_text or v in working_text:
            found_pref = v
            # çœŒåã‚’æ–‡å­—åˆ—ã‹ã‚‰é™¤å»ï¼ˆå¸‚ç”ºæ‘ã®èª¤æ¤œçŸ¥ã‚’é˜²ããŸã‚ï¼‰
            working_text = working_text.replace(v, " ").replace(k, " ")
            break
    
    # 3. å¸‚åŒºç”ºæ‘ã®ç‰¹å®šã¨æŠ½å‡º
    found_city = ""
    
    # æ¼¢å­—ãƒ»ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãŒ1ã€œ6æ–‡å­—ç¶šãã€æœ€å¾Œã«ã€Œå¸‚ãƒ»åŒºãƒ»ç”ºãƒ»æ‘ã€ãŒæ¥ã‚‹ã‚‚ã®ã‚’å…¨æ¤œç´¢
    # ä¾‹: "ç§‘å­¦åšç‰©é¤¨å¯Œå±±å¸‚" -> "åšç‰©é¤¨å¯Œå±±å¸‚" (6æ–‡å­—) ãªã©ãŒãƒãƒƒãƒã™ã‚‹å¯èƒ½æ€§ã‚ã‚Š
    matches = re.findall(r'([ä¸€-é¾ ã-ã‚“ã‚¡-ãƒ³]{1,6}(?:å¸‚|åŒº|ç”º|æ‘))', working_text)
    
    if matches:
        candidates = [m for m in matches if len(m) >= 2]
        
        if candidates:
            # å„ªå…ˆåº¦: å¸‚ > åŒº > ç”º > æ‘
            city_candidates = [m for m in candidates if m.endswith("å¸‚")]
            ward_candidates = [m for m in candidates if m.endswith("åŒº")]
            town_candidates = [m for m in candidates if m.endswith("ç”º") or m.endswith("æ‘")]
            
            # â˜…ãƒ«ãƒ¼ãƒ«é©ç”¨: å¸‚ãŒã‚ã‚‹å ´åˆã¯å¸‚ã®ã¿æ¡ç”¨ã€‚åŒºã‚„ç”ºæ‘ã¯ç„¡è¦–ã€‚
            if city_candidates:
                # "ç§‘å­¦åšç‰©é¤¨å¯Œå±±å¸‚" ã®ã‚ˆã†ãªãƒã‚¤ã‚ºæ··å…¥å¯¾ç­–
                # æœ«å°¾ãŒã€Œå¸‚ã€ã®å€™è£œã®ã†ã¡ã€æœ€ã‚‚ãã‚Œã‚‰ã—ã„ï¼ˆçŸ­ã„ã€ã‹ã¤ã€Œé¤¨ã€ã€Œåœ’ã€ãªã©ã§å§‹ã¾ã‚‰ãªã„ï¼‰ã‚‚ã®ã‚’æ¢ã™
                best_city = city_candidates[0]
                
                # ã‚‚ã—å€™è£œã®ä¸­ã«ã€Œé¤¨ã€ã‚„ã€Œåœ’ã€ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰ã€ãã®æ–‡å­—ä»¥é™ã‚’æ¡ç”¨ã™ã‚‹
                # ä¾‹: "åšç‰©é¤¨å¯Œå±±å¸‚" -> "å¯Œå±±å¸‚"
                for noise in ['é¤¨', 'åœ’', 'æ‰€', 'å ´', 'æ ¡', 'å±€']:
                    if noise in best_city:
                        parts = best_city.split(noise)
                        if len(parts) > 1 and len(parts[-1]) >= 2: # åˆ†å‰²å¾Œã‚‚2æ–‡å­—ä»¥ä¸Šãªã‚‰æ¡ç”¨
                            best_city = parts[-1]
                
                found_city = best_city
            
            elif ward_candidates:
                found_city = ward_candidates[0]
            elif town_candidates:
                found_city = town_candidates[0]
            else:
                found_city = candidates[0]

    # 4. å†æ§‹ç¯‰
    if found_pref and found_city:
        return f"{found_pref}{found_city}"
    
    # çœŒã ã‘è¦‹ã¤ã‹ã£ãŸå ´åˆ
    if found_pref:
        clean_remains = working_text.strip()
        # æ®‹ã‚ŠãŒã‚·ãƒ³ãƒ—ãƒ«ã§çŸ­ã„ãªã‚‰çµåˆã€ãã†ã§ãªã‘ã‚Œã°çœŒã®ã¿
        if clean_remains and len(clean_remains) < 20 and not re.search(r'[0-9]', clean_remains):
             return f"{found_pref}{clean_remains}"
        return found_pref
        
    # å¸‚ã ã‘è¦‹ã¤ã‹ã£ãŸå ´åˆ
    if found_city:
        return found_city

    return working_text.strip() or raw_address

def get_clean_address(props: dict) -> str:
    """Geoapifyã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‹ã‚‰ç¶ºéº—ãªæ—¥æœ¬èªä½æ‰€ã‚’ç”Ÿæˆã™ã‚‹"""
    state = props.get('state', '')
    if state in ['NN', 'Other', 'Others', 'ãã®ä»–', 'JP', 'Japan']: state = ''

    city = props.get('city', '') or props.get('town', '') or props.get('village', '') or props.get('municipality', '')
    if city in ['NN', 'Other', 'Others', 'ãã®ä»–']: city = ''
    
    # â˜…éƒ¡ (county) ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„
    # county = props.get('county', '') 
    
    ward = props.get('suburb', '') or props.get('district', '') 
    if ward in ['NN', 'Other', 'Others', 'ãã®ä»–']: ward = ''
    
    formatted = props.get("formatted", "")

    # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ãŒæƒã£ã¦ã„ã‚‹å ´åˆã¯çµåˆã—ã¦ä½œæˆ
    if state and (city or ward):
        if state in PREF_NORMALIZER:
            state = PREF_NORMALIZER[state]
        elif not any(state.endswith(s) for s in ['éƒ½', 'é“', 'åºœ', 'çœŒ']):
            if state == 'æ±äº¬': state += 'éƒ½'
            elif state in ['äº¬éƒ½', 'å¤§é˜ª']: state += 'åºœ'
            elif state != 'åŒ—æµ·é“': state += 'çœŒ'
            
        # çœŒåã®é‡è¤‡é˜²æ­¢
        state_core = state
        for suffix in ['éƒ½', 'åºœ', 'çœŒ']:
            if state_core.endswith(suffix): state_core = state_core[:-1]
        
        if city and not any(city.endswith(s) for s in ["å¸‚", "åŒº", "ç”º", "æ‘"]):
            city += "å¸‚"

        address_parts = [state]
        
        # â˜…ãƒ«ãƒ¼ãƒ«é©ç”¨: å¸‚ãŒã‚ã‚‹å ´åˆã¯å¸‚ã®ã¿ã€‚åŒºã¯è¿½åŠ ã—ãªã„ã€‚
        if city:
            address_parts.append(city)
        elif ward:
            address_parts.append(ward)
        
        return "".join(address_parts)

    # ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ä¸å‚™æ™‚ã¯ formatted æ–‡å­—åˆ—ã‚’å¼·åŠ›ãªæ­£è¦åŒ–é–¢æ•°ã«é€šã™
    clean_formatted = formatted.replace("NN", "").replace(" ,", "").replace(", ", "").strip()
    return extract_and_fix_address(clean_formatted)

# ---------------------------------------------------------
# å¤–éƒ¨APIé€£æºé–¢æ•° (å¤‰æ›´ãªã—)
# ---------------------------------------------------------
async def fetch_with_retry(client, url, params=None, headers=None, retries=5, initial_timeout=10.0):
    current_timeout = initial_timeout
    wait_time = 1.0
    for attempt in range(retries + 1):
        try:
            res = await client.get(url, params=params, headers=headers, timeout=current_timeout)
            if res.status_code != 429 and res.status_code < 500:
                return res
            print(f"âš ï¸ API Busy (Status: {res.status_code}). Retrying... ({attempt+1}/{retries})")
        except (httpx.TimeoutException, httpx.ConnectError, httpx.ReadError, httpx.PoolTimeout) as e:
            print(f"â³ Timeout/Network Error: {e}. Retrying... ({attempt+1}/{retries})")
        if attempt < retries:
            await asyncio.sleep(wait_time)
            wait_time *= 1.5
            current_timeout += 5.0
        else:
            print(f"âŒ Max retries reached for {url}")
            if 'res' in locals(): return res
            return None

async def fetch_wikipedia_image(client, query: str):
    info = await fetch_wikipedia_info(client, query)
    return info.get("image_url")

async def fetch_wikipedia_info(client, query: str, target_name: str = None):
    if not query: return {"image_url": None, "summary": None}
    
    cache_key = f"wiki_info_v4:{query}"
    cached = get_cache(cache_key)
    if cached: return cached

    try:
        search_url = "https://ja.wikipedia.org/w/api.php"
        search_params = {
            "action": "query", "list": "search", "srsearch": query,
            "format": "json", "utf8": 1, "srlimit": 5 
        }
        res = await fetch_with_retry(client, search_url, params=search_params, headers=WIKI_HEADERS, initial_timeout=3.0)
        
        if not res: return {"image_url": None, "summary": None}
        data = res.json()
        
        search_results = data.get("query", {}).get("search", [])
        if not search_results:
             return {"image_url": None, "summary": None}
             
        page_id = None
        if target_name:
            norm_target = target_name.replace(" ", "").replace("ã€€", "")
            for item in search_results:
                title = item["title"].replace(" ", "").replace("ã€€", "")
                if norm_target in title or title in norm_target:
                    page_id = item["pageid"]
                    break
        else:
            page_id = search_results[0]["pageid"]

        if not page_id:
             return {"image_url": None, "summary": None}
        
        info_url = "https://ja.wikipedia.org/w/api.php"
        info_params = {
            "action": "query", "prop": "pageimages|extracts", "pageids": page_id, 
            "pithumbsize": 500, "exintro": 1, "explaintext": 1, "exchars": 200, "format": "json"
        }
        info_res = await fetch_with_retry(client, info_url, params=info_params, headers=WIKI_HEADERS, initial_timeout=3.0)
        
        if not info_res: return {"image_url": None, "summary": None}
        
        info_data = info_res.json()
        pages = info_data.get("query", {}).get("pages", {})
        page = pages.get(str(page_id))
        
        if page:
            image_url = page.get("thumbnail", {}).get("source")
            summary = page.get("extract", "").replace("\n", "")
            if "å‚ç…§" in summary or "æ›–æ˜§ã•å›é¿" in summary: summary = None
            if summary and len(summary) >= 200: summary = summary.rstrip("ã€ã€‚") + "..."
            
            result = {"image_url": image_url, "summary": summary}
            set_cache(cache_key, result)
            return result
    except Exception as e:
        print(f"Wiki info fetch error: {e}")
    return {"image_url": None, "summary": None}

async def fetch_spot_coordinates(client, target_name: str, search_query: str):
    cache_key = f"geo_v5:{target_name}:{search_query}"
    cached = get_cache(cache_key)
    if cached: return cached

    try:
        clean_query = re.sub(r'[(ï¼ˆ].*?[)ï¼‰]', '', search_query).strip()
        url = "https://api.geoapify.com/v1/geocode/search"
        params = {"text": clean_query, "apiKey": GEOAPIFY_API_KEY, "lang": "ja", "limit": 3, "countrycode": "jp"}
        res = await fetch_with_retry(client, url, params=params, initial_timeout=8.0, retries=5)

        image_url = None
        wiki_summary = None
        
        if res and res.status_code == 200:
            data = res.json()
            if "features" in data and len(data["features"]) > 0:
                for i, feat in enumerate(data["features"]):
                    props = feat["properties"]
                    result_name = props.get("name", "")
                    if not result_name or not result_name.strip(): continue

                    # â˜… ä½æ‰€ã®æ­£è¦åŒ–å‘¼ã³å‡ºã— (éƒ¡å‰Šé™¤ãƒ»å¸‚å„ªå…ˆãƒ­ã‚¸ãƒƒã‚¯é©ç”¨)
                    desc = get_clean_address(props)
                    if not desc: desc = "ä½æ‰€ä¸æ˜"

                    state = props.get("state", "")
                    city = props.get("city", "") or props.get("town", "")
                    wiki_query = f"{result_name} {state}".strip()
                    if len(wiki_query) < len(result_name) + 2: wiki_query = search_query

                    try:
                        wiki_info = await fetch_wikipedia_info(client, wiki_query, target_name=result_name)
                        image_url = wiki_info.get("image_url")
                        if wiki_info.get("summary"): wiki_summary = wiki_info["summary"]
                    except: pass

                    result_data = {
                        "name": result_name, "description": desc, 
                        "coordinates": feat["geometry"]["coordinates"],
                        "image_url": image_url, "comment": wiki_summary or "" 
                    }
                    set_cache(cache_key, result_data)
                    return result_data
    except Exception as e:
        print(f"Coord fetch failed for {target_name}: {e}")
    return None

async def fetch_spot_by_coordinates(client, lat: float, lng: float, fallback_name: str):
    lat_k = round(lat, 6)
    lng_k = round(lng, 6)
    cache_key = f"geo_reverse_v2:{lat_k}:{lng_k}"
    
    cached = get_cache(cache_key)
    if cached: return cached

    try:
        url = "https://api.geoapify.com/v1/geocode/reverse"
        params = {"lat": lat, "lon": lng, "apiKey": GEOAPIFY_API_KEY, "lang": "ja", "limit": 1}
        res = await fetch_with_retry(client, url, params=params, initial_timeout=8.0, retries=3)

        image_url = None
        wiki_summary = None
        
        if res and res.status_code == 200:
            data = res.json()
            if "features" in data and len(data["features"]) > 0:
                props = data["features"][0]["properties"]
                result_name = props.get("name", "")
                if not result_name: result_name = fallback_name

                # â˜… ä½æ‰€ã®æ­£è¦åŒ–å‘¼ã³å‡ºã—
                desc = get_clean_address(props)
                desc = re.sub(r'ã€’\d{3}-\d{4}', '', desc).strip()
                if not desc: desc = "ä½æ‰€ä¸æ˜"

                state = props.get("state", "")
                city = props.get("city", "") or props.get("town", "")
                wiki_query = f"{result_name} {state} {city}".strip()

                try:
                    wiki_info = await fetch_wikipedia_info(client, wiki_query, target_name=result_name)
                    image_url = wiki_info.get("image_url")
                    if wiki_info.get("summary"): wiki_summary = wiki_info["summary"]
                except: pass

                result_data = {
                    "name": result_name, "description": desc,
                    "coordinates": [lng, lat], "image_url": image_url,
                    "comment": wiki_summary or "" 
                }
                set_cache(cache_key, result_data)
                return result_data
    except Exception as e:
        print(f"Reverse Geo Error: {e}")
    return None

# ==========================================
# ğŸ§  AIãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ==========================================
async def get_official_name_by_ai(query: str) -> str:
    cache_key = f"query_norm_v1:{query}"
    cached = get_cache(cache_key)
    if cached: return cached

    prompt = f"""
    ã‚¿ã‚¹ã‚¯: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¤œç´¢èªå¥ã€Œ{query}ã€ã‚’ã€Google Mapsã‚„ãƒŠãƒ“ã§æ¤œç´¢ã—ãŸéš›ã«æœ€ã‚‚ãƒ’ãƒƒãƒˆã—ã‚„ã™ã„ã€æ­£å¼åç§°ã€ã¾ãŸã¯ã€æ¼¢å­—è¡¨è¨˜ã€ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
    ãƒ«ãƒ¼ãƒ«: ä½™è¨ˆãªèª¬æ˜ã¯ä¸€åˆ‡ä¸è¦ã€‚ä¿®æ­£å¾Œã®å˜èªã®ã¿ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚
    ä¾‹: ãã‚‰ã¦ã‚‰ã™ -> SORA terrace
    """
    try:
        res = await aclient.chat.completions.create(
            model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}], max_tokens=50, temperature=0.0
        )
        normalized_name = res.choices[0].message.content.strip().replace('"', '').replace("ã€Œ", "").replace("ã€", "")
        set_cache(cache_key, normalized_name)
        return normalized_name
    except: return query

async def get_structured_address_by_ai(name: str, raw_address: str = "") -> Dict[str, str]:
    cache_key = f"address_fix_v2:{name}:{raw_address}"
    cached = get_cache(cache_key)
    if cached: return cached

    prompt = f"""
    ã‚¿ã‚¹ã‚¯: ã‚¹ãƒãƒƒãƒˆã€Œ{name}ã€ã®æ­£ç¢ºãªä½æ‰€ã‚’ç‰¹å®šã—ã€éƒ½é“åºœçœŒã¨å¸‚åŒºç”ºæ‘ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚
    ç¾åœ¨ã®ä¸å®Œå…¨ãªä½æ‰€æƒ…å ±: {raw_address}
    ãƒ«ãƒ¼ãƒ«:
    1. å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã®ã¿: {{ "prefecture": "ã€‡ã€‡çœŒ", "city": "ã€‡ã€‡å¸‚", "full_address": "ã€‡ã€‡çœŒã€‡ã€‡å¸‚..." }}
    2. "city" ã¯å¸‚ãƒ»åŒºãƒ»ç”ºãƒ»æ‘ã¾ã§ã€‚
    """
    try:
        res = await aclient.chat.completions.create(
            model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}], response_format={"type": "json_object"}, temperature=0.0
        )
        data = json.loads(res.choices[0].message.content)
        set_cache(cache_key, data)
        return data
    except:
        return {"prefecture": "", "city": "", "full_address": raw_address}

# ---------------------------------------------------------
# API: å„ç¨®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ---------------------------------------------------------
@app.post("/api/nearby_spots")
async def nearby_spots(req: NearbyRequest):
    global http_client
    if http_client is None: return {"spots": []}
    client = http_client
    lat_k = round(req.latitude, 3)
    lon_k = round(req.longitude, 3)
    cache_key = f"nearby_v4:{lat_k}:{lon_k}:{req.radius}:{req.mode}"
    cached = get_cache(cache_key)
    if cached: return cached
    try:
        url = "https://api.geoapify.com/v2/places"
        if req.mode == "wide": categories = "commercial.shopping_mall,catering.restaurant,entertainment,leisure.park"
        else: categories = "tourism,building.historic,natural,entertainment.culture,religion"
        params = {
            "categories": categories, "filter": f"circle:{req.longitude},{req.latitude},{req.radius}",
            "bias": f"proximity:{req.longitude},{req.latitude}", "limit": 20, "apiKey": GEOAPIFY_API_KEY, "lang": "ja"
        }
        res = await fetch_with_retry(client, url, params=params, initial_timeout=10.0)
        base_spots = []
        if res and res.status_code == 200:
            data = res.json()
            if "features" in data:
                for feat in data["features"]:
                    props = feat["properties"]
                    name = props.get("name", "")
                    if not name: continue 
                    coords = feat.get("geometry", {}).get("coordinates")
                    if not coords: continue
                    formatted = get_clean_address(props) # â˜…ã“ã“ã§ã‚‚é©ç”¨
                    search_query = f"{name} {props.get('state', '')}".strip()
                    base_spots.append({
                        "id": f"nearby-{props.get('place_id')}", "name": name, "description": formatted, 
                        "coordinates": coords, "is_nearby": True, "search_query": search_query, "image_url": None, "comment": "" 
                    })
        # Wiki enrich
        async def enrich(s):
            try:
                w = await fetch_wikipedia_info(client, s["search_query"], target_name=s["name"])
                if w["image_url"]: s["image_url"] = w["image_url"]
                if w["summary"]: s["comment"] = w["summary"]
            except: pass
            return s
        enriched_spots = await asyncio.gather(*[enrich(s) for s in base_spots]) if base_spots else []
        result = {"spots": enriched_spots}
        if enriched_spots: set_cache(cache_key, result)
        return result
    except: return {"spots": []}

@app.get("/api/get_spot_image")
async def get_spot_image(query: str):
    global http_client
    if http_client is None: return {"image_url": None}
    img_url = await fetch_wikipedia_image(http_client, query)
    return {"image_url": img_url}

@app.post("/api/import_rakuten_hotel")
async def import_rakuten_hotel(req: ImportRequest):
    if not RAKUTEN_APP_ID: 
        return {"error": "ã‚µãƒ¼ãƒãƒ¼ã®æ¥½å¤©APIã‚­ãƒ¼è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"}
    hotel_no = None
    final_url = req.url.strip()
    global http_client
    if http_client is None: 
        return {"error": "ã‚µãƒ¼ãƒãƒ¼ã‚’æº–å‚™ä¸­ã§ã™ã€‚å°‘ã—å¾…ã£ã¦ã‹ã‚‰å†åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"}
    client = http_client

    try:
        cache_key = f"rakuten_import_v3:{final_url}"
        cached = get_cache(cache_key)
        if cached: return cached

        if "rakuten.co.jp" in final_url:
            try:
                res = await fetch_with_retry(client, final_url, initial_timeout=10.0)
                if res: final_url = str(res.url)
            except: pass
        
        path_patterns = [r'hotelinfo/plan/(\d+)', r'HOTEL/(\d+)', r'hotel/(\d+)', r'travel\.rakuten\.co\.jp/.*?/(\d+)']
        for pattern in path_patterns:
            match = re.search(pattern, final_url, re.IGNORECASE)
            if match:
                hotel_no = match.group(1)
                break
        if not hotel_no:
            parsed = urllib.parse.urlparse(final_url)
            qs = urllib.parse.parse_qs(parsed.query)
            for key in ["f_no", "no", "hotelNo", "hotel_no"]:
                if key in qs:
                    hotel_no = qs[key][0]; break
        if not hotel_no: return {"error": "URLã‹ã‚‰ãƒ›ãƒ†ãƒ«IDã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"}

        params = {"applicationId": RAKUTEN_APP_ID, "format": "json", "hotelNo": hotel_no, "datumType": 1}
        api_url = "https://app.rakuten.co.jp/services/api/Travel/SimpleHotelSearch/20170426"
        res = await fetch_with_retry(client, api_url, params=params, initial_timeout=15.0)
        
        if not res or res.status_code != 200: return {"error": f"æ¥½å¤©APIã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ (ID: {hotel_no})"}
        data = res.json()
        if "hotels" not in data or not data["hotels"]: return {"error": "è©²å½“ã™ã‚‹ãƒ›ãƒ†ãƒ«æƒ…å ±ãŒæ¥½å¤©APIã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"}

        raw_hotel = data["hotels"][0]
        hotel_content = raw_hotel["hotel"] if "hotel" in raw_hotel else raw_hotel
        basic = None
        user_review = {}
        if isinstance(hotel_content, list):
            for item in hotel_content:
                if "hotelBasicInfo" in item: basic = item["hotelBasicInfo"]
                if "userReview" in item: user_review = item["userReview"]
                if "hotelRatingInfo" in item: user_review = item["hotelRatingInfo"]
        else: basic = hotel_content.get("hotelBasicInfo")

        if not basic: return {"error": "ãƒ›ãƒ†ãƒ«æƒ…å ±ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"}
        address = f"{basic.get('address1', '')}{basic.get('address2', '')}"
        # â˜…ã“ã“ã§ã‚‚ä½æ‰€æ­£è¦åŒ–ã‚’é€šã™
        address = re.sub(r'[ä¸€-é¾ ã-ã‚“ã‚¡-ãƒ³]{1,6}éƒ¡', '', address)

        spot_data = {
            "id": str(basic["hotelNo"]), "name": basic["hotelName"], "description": address, 
            "coordinates": [basic["longitude"], basic["latitude"]], "image_url": basic.get("hotelImageUrl"), 
            "url": basic.get("hotelInformationUrl"), "price": basic.get("hotelMinCharge", 0), 
            "rating": basic.get("reviewAverage", 3.0), "source": "rakuten", "is_hotel": True, "status": "hotel_candidate",
            "comment": basic.get("hotelSpecial", "")[:100] + "..." 
        }
        result = {"spot": spot_data}
        set_cache(cache_key, result)
        return result
    except Exception as e:
        print(f"Import Error: {e}")
        return {"error": f"å–è¾¼å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"}

@app.post("/api/search_hotels_vacant")
async def search_hotels_vacant(req: VacantSearchRequest):
    if not RAKUTEN_APP_ID: return {"error": "ã‚µãƒ¼ãƒãƒ¼è¨­å®šã‚¨ãƒ©ãƒ¼"}
    global http_client
    if http_client is None: return {"error": "Server starting up..."}
    client = http_client
    cache_key = f"rakuten_vacant_v4:{req.latitude}:{req.longitude}:{req.checkin_date}:{req.checkout_date}:{req.adult_num}:{req.min_price}:{req.max_price}:{req.meal_type}:{hashlib.md5(str(req.polygon).encode()).hexdigest() if req.polygon else 'all'}"
    cached = get_cache(cache_key)
    if cached: return cached

    safe_radius = min(round(req.radius, 2), 3.0)
    today = date.today()
    c_in = req.checkin_date or (today + timedelta(days=30)).strftime("%Y-%m-%d")
    c_out = req.checkout_date or (date.fromisoformat(c_in) + timedelta(days=1)).strftime("%Y-%m-%d")

    base_params = {
        "applicationId": RAKUTEN_APP_ID, "format": "json", "latitude": req.latitude, "longitude": req.longitude,
        "searchRadius": safe_radius, "datumType": 1, "hits": 30, "sort": "standard",
        "checkinDate": c_in, "checkoutDate": c_out, "adultNum": req.adult_num,
    }
    if req.max_price: base_params["maxCharge"] = req.max_price
    if req.min_price: base_params["minCharge"] = req.min_price
    if req.meal_type == 'room_only': base_params["breakfastFlag"] = 0; base_params["dinnerFlag"] = 0
    elif req.meal_type == 'breakfast': base_params["breakfastFlag"] = 1; base_params["dinnerFlag"] = 0 
    elif req.meal_type == 'half_board': base_params["breakfastFlag"] = 1; base_params["dinnerFlag"] = 1

    url = "https://app.rakuten.co.jp/services/api/Travel/VacantHotelSearch/20170426"

    async def fetch_page(page_num):
        try:
            p = base_params.copy(); p["page"] = page_num
            res = await fetch_with_retry(client, url, params=p, initial_timeout=15.0, retries=5)
            if res and res.status_code == 200: return res.json()
        except: pass
        return None

    try:
        results = await asyncio.gather(*[fetch_page(i) for i in range(1, 6)])
        all_hotels = []
        seen_ids = set()
        for data in results:
            if not data or "hotels" not in data: continue
            for h_group in data["hotels"]:
                try:
                    hotel_content = h_group["hotel"] if "hotel" in h_group else h_group
                    if not isinstance(hotel_content, list) or len(hotel_content) == 0: continue
                    basic = None
                    for item in hotel_content:
                        if "hotelBasicInfo" in item: basic = item["hotelBasicInfo"]
                    if not basic: continue
                    hotel_id = str(basic["hotelNo"])
                    if hotel_id in seen_ids: continue
                    if req.polygon and not is_inside_polygon(basic["latitude"], basic["longitude"], req.polygon): continue

                    best_price = float('inf'); best_plan_id, best_room_class = None, None; found_valid_plan = False
                    for j in range(1, len(hotel_content)):
                        r_info = hotel_content[j].get("roomInfo")
                        if isinstance(r_info, list) and len(r_info) >= 2:
                            r_basic = r_info[0].get("roomBasicInfo", {})
                            if req.meal_type == 'room_only' and (r_basic.get("withBreakfastFlag") == 1 or r_basic.get("withDinnerFlag") == 1): continue
                            elif req.meal_type == 'breakfast' and r_basic.get("withBreakfastFlag") != 1: continue
                            elif req.meal_type == 'half_board' and (r_basic.get("withBreakfastFlag") != 1 or r_basic.get("withDinnerFlag") != 1): continue
                            r_charge = r_info[1].get("dailyCharge")
                            if r_charge and r_charge.get("total", 0) > 0 and r_charge["total"] < best_price:
                                best_price = r_charge["total"]
                                best_plan_id = r_info[0].get("roomBasicInfo", {}).get("planId")
                                best_room_class = r_info[0].get("roomBasicInfo", {}).get("roomClass")
                                found_valid_plan = True

                    if not found_valid_plan: continue
                    if req.min_price and best_price < req.min_price: continue
                    if req.max_price and best_price > req.max_price: continue
                    
                    address = f"{basic.get('address1', '')}{basic.get('address2', '')}"
                    address = re.sub(r'[ä¸€-é¾ ã-ã‚“ã‚¡-ãƒ³]{1,6}éƒ¡', '', address)

                    all_hotels.append({
                        "id": hotel_id, "name": basic["hotelName"], "description": address, 
                        "coordinates": [basic["longitude"], basic["latitude"]], "image_url": basic.get("hotelImageUrl"), 
                        "url": basic.get("hotelInformationUrl"), "price": int(best_price), "rating": basic.get("reviewAverage") or 0.0,
                        "source": "rakuten", "is_hotel": True, "plan_id": best_plan_id, "room_class": best_room_class, 
                        "status": "hotel_candidate", "comment": basic.get("hotelSpecial", "")[:60] + "..." 
                    })
                    seen_ids.add(hotel_id)
                except: continue
        result = {"hotels": all_hotels}
        if all_hotels: set_cache(cache_key, result)
        return result
    except Exception as e:
        traceback.print_exc()
        return {"error": f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {str(e)}"}

@app.post("/api/suggest_spots")
async def suggest_spots(req: SuggestRequest):
    return StreamingResponse(suggest_spots_generator(req), media_type="application/x-ndjson")

async def suggest_spots_generator(req: SuggestRequest):
    global http_client
    if http_client is None:
        yield json.dumps({"type": "error", "message": "Server starting..."}) + "\n"; return
    client = http_client
    yield json.dumps({"type": "status", "message": "AIãŒå€™è£œåœ°ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ä¸­..."}) + "\n"
    
    existing_names = []
    for item in req.existing_spots:
        if isinstance(item, dict): existing_names.append(item.get("name", ""))
        elif isinstance(item, str): existing_names.append(item)
        elif hasattr(item, "name"): existing_names.append(item.name)

    prompt = f"""
    å ´æ‰€: {req.theme}
    ã‚¿ã‚¹ã‚¯: è¦³å…‰å®¢ã«äººæ°—ã®ã€Œè¶…æœ‰åãƒ»ç‹é“è¦³å…‰ã‚¹ãƒãƒƒãƒˆã€ã‚’äººæ°—é †ã«15å€‹æŒ™ã’ã¦ãã ã•ã„ã€‚
    æ¡ä»¶:
    1. ãƒ›ãƒ†ãƒ«ã‚„å®¿æ³Šæ–½è¨­ã¯é™¤å¤–ã€‚
    2. æ—¢å­˜ãƒªã‚¹ãƒˆ: {", ".join(existing_names)} ã¯çµ¶å¯¾ã«é™¤å¤–ã€‚
    3. å‡ºåŠ›ã¯JSONå½¢å¼ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆé…åˆ—ã¨ã™ã‚‹ã€‚
    4. å„ã‚¹ãƒãƒƒãƒˆ: {{"name": "æ­£å¼åç§°", "search_query": "åœ°å›³æ¤œç´¢ã‚¯ã‚¨ãƒª", "summary": "ä¸€è¨€èª¬æ˜", "category": "ã‚«ãƒ†ã‚´ãƒª"}}
    """
    target_spots = []
    try:
        ai_res = await aclient.chat.completions.create(
            model="gpt-4o-mini", messages=[{"role": "user", "content": prompt}], response_format={"type": "json_object"}, max_tokens=1500
        )
        json_data = json.loads(ai_res.choices[0].message.content)
        raw_spots = json_data.get("spots", [])
        seen_names = set(existing_names)
        for s in raw_spots:
            if s["name"] not in seen_names: target_spots.append(s); seen_names.add(s["name"])
        target_spots = target_spots[:10]
        yield json.dumps({"type": "candidates", "names": [s["name"] for s in target_spots], "message": "ä½ç½®æƒ…å ±ã‚’ç…§åˆä¸­..."}) + "\n"
    except Exception as e:
        yield json.dumps({"type": "error", "message": f"AIç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"}) + "\n"; return

    found_count = 0; seen_coords = []
    async def fetch_and_enrich(spot_info):
        res = await fetch_spot_coordinates(client, spot_info["name"], spot_info["search_query"])
        if res:
            if spot_info.get("summary"): res["comment"] = spot_info.get("summary")
            res["category"] = spot_info.get("category", "è¦³å…‰ã‚¹ãƒãƒƒãƒˆ")
            return res
        return None

    tasks = [fetch_and_enrich(s) for s in target_spots]
    for future in asyncio.as_completed(tasks):
        try:
            res = await future
            if res and res["coordinates"] != [0.0, 0.0]:
                if res["coordinates"] in seen_coords: continue
                seen_coords.append(res["coordinates"])
                found_count += 1
                yield json.dumps({"type": "spot_found", "spot": {**res, "stay_time": 90, "source": "ai", "is_hotel": False, "status": "candidate"}}) + "\n"
        except: continue
    yield json.dumps({"type": "done", "count": found_count}) + "\n"

@app.post("/api/verify_spots")
async def verify_spots(req: VerifyRequest):
    return {"spots": req.spots}

async def calculate_route_fallback(client, ordered_spots, start_min, limit_min):
    if not ordered_spots: return {"error": "ã‚¹ãƒãƒƒãƒˆãŒã‚ã‚Šã¾ã›ã‚“"}
    coords_str = ";".join([f"{s.coordinates[0]:.5f},{s.coordinates[1]:.5f}" for s in ordered_spots[:25]])
    cache_key = f"route:{coords_str}:{start_min}:{limit_min}"
    cached = get_cache(cache_key)
    if cached: return cached
    calc_spots = ordered_spots[:25]
    request_coords = ";".join([f"{s.coordinates[0]},{s.coordinates[1]}" for s in calc_spots])
    url = f"https://api.mapbox.com/directions/v5/mapbox/driving/{request_coords}"
    res = await fetch_with_retry(client, url, params={"access_token": MAPBOX_ACCESS_TOKEN, "geometries": "geojson"}, initial_timeout=10.0, retries=5)
    if not res: return {"error": "ãƒ«ãƒ¼ãƒˆè¨ˆç®—APIã¸ã®æ¥ç¶šå¤±æ•—"}
    data = res.json()
    if "routes" not in data or not data['routes']: return {"error": "ãƒ«ãƒ¼ãƒˆè¨ˆç®—å¤±æ•—"}
    route = data['routes'][0]; timeline = []; current = start_min
    for i, spot in enumerate(calc_spots):
        stay = spot.stay_time or 60; arr = current; dep = arr + stay
        timeline.append({"type": "spot", "spot": {**spot.model_dump(), "stay_time": stay}, "arrival": f"{int(arr//60):02d}:{int(arr%60):02d}", "departure": f"{int(dep//60):02d}:{int(dep%60):02d}"})
        if i < len(calc_spots) - 1:
            dur = 30
            if i < len(route.get('legs', [])): dur = math.ceil(route['legs'][i]['duration'] / 60)
            if i+1 < len(calc_spots):
                gurl = f"http://googleusercontent.com/maps.google.com/?saddr={urllib.parse.quote(spot.name)}&daddr={urllib.parse.quote(calc_spots[i+1].name)}&travelmode=driving"
                timeline.append({"type": "travel", "duration_min": dur, "transport_mode": "car", "google_maps_url": gurl})
            current = dep + dur
    used = set(t['spot']['name'] for t in timeline if t['type']=='spot')
    result = {"timeline": timeline, "unused_spots": [s for s in ordered_spots if s.name not in used], "route_geometry": route['geometry']}
    set_cache(cache_key, result)
    return result

@app.post("/api/optimize_route")
@app.post("/api/calculate_route")
async def calculate_route_endpoint(req: OptimizeRequest):
    spots = [s for s in req.spots if s.coordinates and len(s.coordinates) >= 2]
    if len(spots) < 2: return {"error": "2ç®‡æ‰€ä»¥ä¸Šå¿…è¦"}
    global http_client
    if http_client is None: return {"error": "Server starting up..."}
    try: sh, sm = map(int, req.start_time.split(':')); eh, em = map(int, req.end_time.split(':')); start, limit = sh*60+sm, eh*60+em
    except: start, limit = 540, 1080
    return await calculate_route_fallback(http_client, spots, start, limit)

@app.get("/api/search_places")
async def search_places(query: str, lat: Optional[float] = None, lng: Optional[float] = None):
    global http_client
    if http_client is None: return {"results": []}
    client = http_client
    cache_key_raw = f"search_places_smart_v2:{query}:{lat}:{lng}"
    cached_raw = get_cache(cache_key_raw)
    if cached_raw: return cached_raw
    async def execute_search(search_q):
        local_results = []
        try:
            geo_url = "https://api.geoapify.com/v1/geocode/search"
            geo_params = {"text": search_q, "apiKey": GEOAPIFY_API_KEY, "lang": "ja", "limit": 5, "countrycode": "jp"}
            if lat and lng: geo_params["bias"] = f"proximity:{lng},{lat}"
            res = await fetch_with_retry(client, geo_url, params=geo_params, initial_timeout=5.0)
            if res and res.status_code == 200:
                data = res.json()
                for feat in data.get("features", []):
                    props = feat["properties"]
                    name = props.get("name", "") or props.get("formatted", "").split(",")[0]
                    clean_fmt = get_clean_address(props)
                    local_results.append({"id": feat["properties"].get("place_id"), "name": name, "place_name": clean_fmt, "center": feat["geometry"]["coordinates"], "type": "location"})
        except: pass
        if len(local_results) < 3:
            try:
                places_url = "https://api.geoapify.com/v2/places"
                p_params = {"name": search_q, "apiKey": GEOAPIFY_API_KEY, "lang": "ja", "limit": 5}
                if lat and lng: p_params["bias"] = f"proximity:{lng},{lat}"
                res = await fetch_with_retry(client, places_url, params=p_params, initial_timeout=5.0)
                if res and res.status_code == 200:
                    data = res.json()
                    for feat in data.get("features", []):
                        props = feat["properties"]
                        name = props.get("name", ""); 
                        if not name: continue
                        clean_fmt = get_clean_address(props)
                        local_results.append({"id": feat["properties"].get("place_id"), "name": name, "place_name": clean_fmt, "center": feat["geometry"]["coordinates"], "type": "place"})
            except: pass
        return local_results
    results = await execute_search(query)
    response_data = {"results": results}
    set_cache(cache_key_raw, response_data)
    return response_data

@app.get("/api/get_spot_info")
async def get_spot_info(query: str, lat: Optional[float] = None, lng: Optional[float] = None):
    global http_client
    if http_client is None: return {}
    client = http_client
    data = None
    if lat is not None and lng is not None: data = await fetch_spot_by_coordinates(client, lat, lng, query)
    if not data: data = await fetch_spot_coordinates(client, query, query)
    current_desc = data.get("description", "") if data else ""
    has_pref = any(p in current_desc for p in PREF_NORMALIZER.values())
    is_invalid = (not current_desc or "NN" in current_desc or "èª¿æŸ»ä¸­" in current_desc or "ä¸æ˜" in current_desc or not has_pref)
    if is_invalid:
        ai_data = await get_structured_address_by_ai(query, current_desc)
        if ai_data.get("full_address"):
            if not data: data = {"name": query, "coordinates": [lng or 0.0, lat or 0.0]}
            data["description"] = ai_data["full_address"]
    wiki = await fetch_wikipedia_info(client, query, target_name=query)
    if data:
        data["image_url"] = data.get("image_url") or wiki.get("image_url")
        data["comment"] = data.get("comment") or wiki.get("summary") or ""
        return data
    return {"name": query, "description": "", "image_url": wiki.get("image_url"), "comment": wiki.get("summary") or ""}