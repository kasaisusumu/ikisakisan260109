from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, field_validator
from typing import Optional, List, Any, Dict, Union
import os
import json
import urllib.parse
import asyncio
import httpx 
from dotenv import load_dotenv
from openai import AsyncOpenAI 
import math
import re 
import traceback
from datetime import date, timedelta 
import random 
from contextlib import asynccontextmanager
import sqlite3
import hashlib

load_dotenv()

# ==========================================
# ğŸ”‘ è¨­å®š
# ==========================================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MAPBOX_ACCESS_TOKEN = os.getenv("MAPBOX_ACCESS_TOKEN")
GEOAPIFY_API_KEY = os.getenv("GEOAPIFY_API_KEY")
RAKUTEN_APP_ID = os.getenv("RAKUTEN_APP_ID")

# HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
http_client = None

# ==========================================
# ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ  (SQLite)
# ==========================================
DB_PATH = "cache.db"

def init_db():
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS api_cache (
                key TEXT PRIMARY KEY,
                value TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        conn.commit()

def get_cache(key: str) -> Optional[Dict]:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å–å¾—"""
    try:
        with sqlite3.connect(DB_PATH) as conn:
            cursor = conn.execute("SELECT value FROM api_cache WHERE key = ?", (key,))
            row = cursor.fetchone()
            if row:
                return json.loads(row[0])
    except Exception as e:
        print(f"Cache Read Error: {e}")
    return None

def set_cache(key: str, data: Any):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä¿å­˜"""
    try:
        with sqlite3.connect(DB_PATH) as conn:
            conn.execute(
                "INSERT OR REPLACE INTO api_cache (key, value) VALUES (?, ?)",
                (key, json.dumps(data))
            )
            conn.commit()
    except Exception as e:
        print(f"Cache Write Error: {e}")

# ==========================================
# ğŸš€ ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«
# ==========================================
@asynccontextmanager
async def lifespan(app: FastAPI):
    init_db()
    global http_client
    http_client = httpx.AsyncClient(verify=False, timeout=30.0)
    print("âœ… System initialized with SQLite Cache & Robust Retry Logic")
    yield
    if http_client:
        await http_client.aclose()

app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

aclient = AsyncOpenAI(
    api_key=OPENAI_API_KEY,
    max_retries=5,     
    timeout=60.0       
)

# --- å‹å®šç¾© ---
class ExistingSpot(BaseModel):
    name: str
    coordinates: Optional[List[float]] = None

class SearchRequest(BaseModel):
    query: str
    area: str = "" 

class VacantSearchRequest(BaseModel):
    latitude: float
    longitude: float
    radius: float = 3.0
    min_price: Optional[int] = None
    max_price: Optional[int] = None
    squeeze: List[str] = [] 
    checkin_date: Optional[str] = None
    checkout_date: Optional[str] = None
    adult_num: int = 2
    meal_type: Optional[str] = None # 'room_only', 'breakfast', 'half_board', 'none'
    polygon: Optional[List[List[float]]] = None 

class ImportRequest(BaseModel):
    url: str

class SuggestRequest(BaseModel):
    theme: str             
    existing_spots: List[Union[ExistingSpot, str, Dict[str, Any]]] = [] 
    liked_spots: list[str] = []
    noped_spots: list[str] = []
    area: str = ""         
    verify: bool = False   

class Spot(BaseModel):
    name: str
    description: str = ""
    coordinates: List[float]
    votes: int = 0
    stay_time: int = 0
    image_url: Optional[str] = None
    price: Optional[int] = None
    rating: Optional[float] = None
    url: Optional[str] = None
    source: str = "ai" 
    is_jalan: bool = False 
    mapbox_id: Optional[str] = None
    place_formatted: Optional[str] = None
    is_hotel: bool = False
    plan_id: Optional[str] = None
    room_class: Optional[str] = None
    status: str = "candidate"
    day: int = 0
    comment: str = ""
    link: str = ""
    category: str = "è¦³å…‰ã‚¹ãƒãƒƒãƒˆ"

    @field_validator('stay_time', 'votes', mode='before')
    def parse_int_fields(cls, v):
        if v is None: return 0
        if isinstance(v, (str, float)):
            try: return int(float(v))
            except: return 0
        return int(v)

    @field_validator('coordinates', mode='before')
    def parse_coordinates(cls, v):
        if isinstance(v, list):
            try: return [float(x) for x in v]
            except: return [0.0, 0.0]
        return v

class VerifyRequest(BaseModel):
    spots: List[Spot]

class OptimizeRequest(BaseModel):
    spots: List[Spot]
    start_time: str = "09:00" 
    end_time: str = "18:00"
    start_spot_name: Optional[str] = None
    end_spot_name: Optional[str] = None

class NearbyRequest(BaseModel):
    latitude: float
    longitude: float
    radius: int = 3000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3km
    limit: int = 20
    mode: str = "standard" # 'standard' (è¦³å…‰) or 'wide' (å•†æ¥­æ–½è¨­ãƒ»é£Ÿäº‹ç­‰)

# ---------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ---------------------------------------------------------

WIKI_HEADERS = {
    "User-Agent": "RouteHackerBot/1.0 (contact@example.com)"
}

def haversine_distance(coord1, coord2):
    R = 6371  # åœ°çƒã®åŠå¾„ (km)
    if not coord1 or not coord2: return float('inf')
    try:
        lat1, lon1 = math.radians(coord1[1]), math.radians(coord1[0])
        lat2, lon2 = math.radians(coord2[1]), math.radians(coord2[0])
        dlat = lat2 - lat1
        dlon = lon2 - lon1
        a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
        return R * c
    except:
        return float('inf')

def is_inside_polygon(lat, lng, poly_coords):
    # poly_coords is list of [lng, lat]
    inside = False
    j = len(poly_coords) - 1
    for i in range(len(poly_coords)):
        xi, yi = poly_coords[i][0], poly_coords[i][1] # lng, lat
        xj, yj = poly_coords[j][0], poly_coords[j][1]
        
        intersect = ((yi > lat) != (yj > lat)) and \
            (lng < (xj - xi) * (lat - yi) / (yj - yi) + xi)
        if intersect:
            inside = not inside
        j = i
    return inside

# â–¼â–¼â–¼ éƒ½é“åºœçœŒåå¤‰æ›ãƒ»è£œå®Œãƒãƒƒãƒ— â–¼â–¼â–¼
PREF_NORMALIZER = {
    # åŒ—æµ·é“ãƒ»æ±åŒ—
    "åŒ—æµ·é“": "åŒ—æµ·é“", "Hokkaido": "åŒ—æµ·é“",
    "é’æ£®": "é’æ£®çœŒ", "Aomori": "é’æ£®çœŒ",
    "å²©æ‰‹": "å²©æ‰‹çœŒ", "Iwate": "å²©æ‰‹çœŒ",
    "å®®åŸ": "å®®åŸçœŒ", "Miyagi": "å®®åŸçœŒ",
    "ç§‹ç”°": "ç§‹ç”°çœŒ", "Akita": "ç§‹ç”°çœŒ",
    "å±±å½¢": "å±±å½¢çœŒ", "Yamagata": "å±±å½¢çœŒ",
    "ç¦å³¶": "ç¦å³¶çœŒ", "Fukushima": "ç¦å³¶çœŒ",
    # é–¢æ±
    "èŒ¨åŸ": "èŒ¨åŸçœŒ", "Ibaraki": "èŒ¨åŸçœŒ",
    "æ ƒæœ¨": "æ ƒæœ¨çœŒ", "Tochigi": "æ ƒæœ¨çœŒ",
    "ç¾¤é¦¬": "ç¾¤é¦¬çœŒ", "Gunma": "ç¾¤é¦¬çœŒ",
    "åŸ¼ç‰": "åŸ¼ç‰çœŒ", "Saitama": "åŸ¼ç‰çœŒ",
    "åƒè‘‰": "åƒè‘‰çœŒ", "Chiba": "åƒè‘‰çœŒ",
    "æ±äº¬": "æ±äº¬éƒ½", "Tokyo": "æ±äº¬éƒ½",
    "ç¥å¥ˆå·": "ç¥å¥ˆå·çœŒ", "Kanagawa": "ç¥å¥ˆå·çœŒ",
    # ä¸­éƒ¨
    "æ–°æ½Ÿ": "æ–°æ½ŸçœŒ", "Niigata": "æ–°æ½ŸçœŒ",
    "å¯Œå±±": "å¯Œå±±çœŒ", "Toyama": "å¯Œå±±çœŒ",
    "çŸ³å·": "çŸ³å·çœŒ", "Ishikawa": "çŸ³å·çœŒ",
    "ç¦äº•": "ç¦äº•çœŒ", "Fukui": "ç¦äº•çœŒ",
    "å±±æ¢¨": "å±±æ¢¨çœŒ", "Yamanashi": "å±±æ¢¨çœŒ",
    "é•·é‡": "é•·é‡çœŒ", "Nagano": "é•·é‡çœŒ",
    "å²é˜œ": "å²é˜œçœŒ", "Gifu": "å²é˜œçœŒ",
    "é™å²¡": "é™å²¡çœŒ", "Shizuoka": "é™å²¡çœŒ",
    "æ„›çŸ¥": "æ„›çŸ¥çœŒ", "Aichi": "æ„›çŸ¥çœŒ",
    # è¿‘ç•¿
    "ä¸‰é‡": "ä¸‰é‡çœŒ", "Mie": "ä¸‰é‡çœŒ",
    "æ»‹è³€": "æ»‹è³€çœŒ", "Shiga": "æ»‹è³€çœŒ",
    "äº¬éƒ½": "äº¬éƒ½åºœ", "Kyoto": "äº¬éƒ½åºœ",
    "å¤§é˜ª": "å¤§é˜ªåºœ", "Osaka": "å¤§é˜ªåºœ",
    "å…µåº«": "å…µåº«çœŒ", "Hyogo": "å…µåº«çœŒ",
    "å¥ˆè‰¯": "å¥ˆè‰¯çœŒ", "Nara": "å¥ˆè‰¯çœŒ",
    "å’Œæ­Œå±±": "å’Œæ­Œå±±çœŒ", "Wakayama": "å’Œæ­Œå±±çœŒ",
    # ä¸­å›½
    "é³¥å–": "é³¥å–çœŒ", "Tottori": "é³¥å–çœŒ",
    "å³¶æ ¹": "å³¶æ ¹çœŒ", "Shimane": "å³¶æ ¹çœŒ",
    "å²¡å±±": "å²¡å±±çœŒ", "Okayama": "å²¡å±±çœŒ",
    "åºƒå³¶": "åºƒå³¶çœŒ", "Hiroshima": "åºƒå³¶çœŒ",
    "å±±å£": "å±±å£çœŒ", "Yamaguchi": "å±±å£çœŒ",
    # å››å›½
    "å¾³å³¶": "å¾³å³¶çœŒ", "Tokushima": "å¾³å³¶çœŒ",
    "é¦™å·": "é¦™å·çœŒ", "Kagawa": "é¦™å·çœŒ",
    "æ„›åª›": "æ„›åª›çœŒ", "Ehime": "æ„›åª›çœŒ",
    "é«˜çŸ¥": "é«˜çŸ¥çœŒ", "Kochi": "é«˜çŸ¥çœŒ",
    # ä¹å·ãƒ»æ²–ç¸„
    "ç¦å²¡": "ç¦å²¡çœŒ", "Fukuoka": "ç¦å²¡çœŒ",
    "ä½è³€": "ä½è³€çœŒ", "Saga": "ä½è³€çœŒ",
    "é•·å´": "é•·å´çœŒ", "Nagasaki": "é•·å´çœŒ",
    "ç†Šæœ¬": "ç†Šæœ¬çœŒ", "Kumamoto": "ç†Šæœ¬çœŒ",
    "å¤§åˆ†": "å¤§åˆ†çœŒ", "Oita": "å¤§åˆ†çœŒ",
    "å®®å´": "å®®å´çœŒ", "Miyazaki": "å®®å´çœŒ",
    "é¹¿å…å³¶": "é¹¿å…å³¶çœŒ", "Kagoshima": "é¹¿å…å³¶çœŒ",
    "æ²–ç¸„": "æ²–ç¸„çœŒ", "Okinawa": "æ²–ç¸„çœŒ"
}

# â–¼â–¼â–¼ ä½æ‰€æ•´å½¢ãƒ­ã‚¸ãƒƒã‚¯ (åˆ†å‰²ãƒ»æ­£è¦åŒ–ãƒ»çµåˆ) â–¼â–¼â–¼

# main.py ã® 200è¡Œç›®ä»˜è¿‘ã‚’ä»¥ä¸‹ã«å·®ã—æ›¿ãˆ

# main.py ã® extract_and_fix_address ã‚’ã•ã‚‰ã«å¼·åŒ–

def extract_and_fix_address(raw_address: str) -> str:
    if not raw_address: return ""

    # 1. ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆæ—¥æœ¬ã€éƒµä¾¿ç•ªå·ã€ä¸è¦ãªç©ºç™½ã‚’å‰Šé™¤ï¼‰
    raw_address = re.sub(r'^(Japan|æ—¥æœ¬|ã€’?\d{3}-\d{4})\s*[, ]*', '', raw_address).strip()

    # 2. éƒ½é“åºœçœŒã®è£œå®Œã¨å¸‚åŒºç”ºæ‘ã®åˆ†é›¢ã‚’è©¦ã¿ã‚‹
    # PREF_NORMALIZER: {"å¯Œå±±": "å¯Œå±±çœŒ", ...}
    for k, v in PREF_NORMALIZER.items():
        # "å¯Œå±±" ãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŒ "å¯Œå±±çœŒ"ï¼ˆæ­£å¼åç§°ï¼‰ã«ãªã£ã¦ã„ãªã„å ´åˆ
        if k in raw_address and v not in raw_address:
            # éƒ½é“åºœçœŒã®å¾Œã«ç¶šãæ–‡å­—åˆ—ã‚’ã€Œå¸‚åŒºç”ºæ‘ä»¥é™ã€ã¨ã—ã¦åˆ‡ã‚Šå‡ºã™
            # ä¾‹: "å¯Œå±±é«˜å²¡å¸‚" -> k="å¯Œå±±", v="å¯Œå±±çœŒ"
            rest = raw_address.split(k, 1)[-1]
            
            # ã‚‚ã—åˆ‡ã‚Šå‡ºã—ãŸå¾Œã®å…ˆé ­ãŒã€ŒçœŒã€ãªã©ã®é‡è¤‡ãªã‚‰é™¤å»ï¼ˆ"å¯Œå±±çœŒé«˜å²¡å¸‚"ã®ã‚±ãƒ¼ã‚¹å¯¾ç­–ï¼‰
            rest = re.sub(r'^[çœŒéƒ½åºœé“]', '', rest)
            
            # å†æ§‹ç¯‰: "å¯Œå±±çœŒ" + "é«˜å²¡å¸‚"
            raw_address = v + rest
            break

    # 3. å¸‚åŒºç”ºæ‘ãŒæŠœã‘ã¦ã„ã‚‹å ´åˆã®æ¨è«–ï¼ˆä¾‹: "å¯Œå±±çœŒ" ã ã‘ã®å ´åˆã¸ã®å¯¾ç­–ï¼‰
    # å¸‚ãƒ»åŒºãƒ»éƒ¡ãƒ»ç”ºãƒ»æ‘ ã®ã„ãšã‚Œã‚‚å«ã¾ã‚Œã¦ã„ãªã„å ´åˆ
    delimiters = ["å¸‚", "åŒº", "éƒ¡", "ç”º", "æ‘"]
    if not any(d in raw_address for d in delimiters):
        # AIè£œå®Œï¼ˆget_structured_address_by_aiï¼‰ã«å›ã™ãƒ•ãƒ©ã‚°ã‚’ç«‹ã¦ã‚‹ã‹ã€
        # ã“ã“ã§æœ€å°é™ã®æ•´å½¢ã‚’è©¦ã¿ã‚‹
        pass

    return raw_address

def get_clean_address(props: dict) -> str:
    """Geoapifyã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‹ã‚‰ç¶ºéº—ãªæ—¥æœ¬èªä½æ‰€ã‚’ç”Ÿæˆã™ã‚‹"""
    state = props.get('state', '')
    if state in ['NN', 'Other', 'Others', 'ãã®ä»–', 'JP', 'Japan']: state = ''

    city = props.get('city', '') or props.get('town', '') or props.get('village', '') or props.get('municipality', '')
    if city in ['NN', 'Other', 'Others', 'ãã®ä»–']: city = ''
    
    county = props.get('county', '') 
    if county in ['NN', 'Other', 'Others', 'ãã®ä»–']: county = ''
    
    ward = props.get('suburb', '') or props.get('district', '') 
    if ward in ['NN', 'Other', 'Others', 'ãã®ä»–']: ward = ''
    
    formatted = props.get("formatted", "")

    if state and (city or county or ward):
        if state in PREF_NORMALIZER:
            state = PREF_NORMALIZER[state]
        elif not any(state.endswith(s) for s in ['éƒ½', 'é“', 'åºœ', 'çœŒ']):
            if state == 'æ±äº¬': state += 'éƒ½'
            elif state in ['äº¬éƒ½', 'å¤§é˜ª']: state += 'åºœ'
            elif state != 'åŒ—æµ·é“': state += 'çœŒ'
            
        state_core = state
        if state_core.endswith("éƒ½"): state_core = state_core[:-1]
        elif state_core.endswith("åºœ"): state_core = state_core[:-1]
        elif state_core.endswith("çœŒ"): state_core = state_core[:-1]
        
        if city == state_core:
             city += "å¸‚"

        address_parts = [state]
        if county and not city: address_parts.append(county)
        if city: address_parts.append(city)
        if ward: address_parts.append(ward)
        
        return "".join(address_parts)

    clean_formatted = formatted.replace("NN", "").replace(" ,", "").replace(", ", "").strip()
    return extract_and_fix_address(clean_formatted)

async def fetch_with_retry(client, url, params=None, headers=None, retries=5, initial_timeout=10.0):
    current_timeout = initial_timeout
    wait_time = 1.0
    for attempt in range(retries + 1):
        try:
            res = await client.get(url, params=params, headers=headers, timeout=current_timeout)
            if res.status_code != 429 and res.status_code < 500:
                return res
            print(f"âš ï¸ API Busy (Status: {res.status_code}). Retrying... ({attempt+1}/{retries})")
        except (httpx.TimeoutException, httpx.ConnectError, httpx.ReadError, httpx.PoolTimeout) as e:
            print(f"â³ Timeout/Network Error: {e}. Retrying... ({attempt+1}/{retries})")
        if attempt < retries:
            await asyncio.sleep(wait_time)
            wait_time *= 1.5
            current_timeout += 5.0
        else:
            print(f"âŒ Max retries reached for {url}")
            if 'res' in locals(): return res
            return None

async def fetch_wikipedia_image(client, query: str):
    """(æ—§äº’æ›) ç”»åƒURLã®ã¿ã‚’å–å¾—"""
    info = await fetch_wikipedia_info(client, query)
    return info.get("image_url")

async def fetch_wikipedia_info(client, query: str, target_name: str = None):
    """ç”»åƒã¨æ¦‚è¦(summary)ã‚’ã¾ã¨ã‚ã¦å–å¾—ã™ã‚‹"""
    if not query: return {"image_url": None, "summary": None}
    
    cache_key = f"wiki_info_v4:{query}"
    cached = get_cache(cache_key)
    if cached: return cached

    try:
        search_url = "https://ja.wikipedia.org/w/api.php"
        search_params = {
            "action": "query", "list": "search", "srsearch": query,
            "format": "json", "utf8": 1, "srlimit": 5 
        }
        res = await fetch_with_retry(client, search_url, params=search_params, headers=WIKI_HEADERS, initial_timeout=3.0)
        
        if not res: return {"image_url": None, "summary": None}
        data = res.json()
        
        search_results = data.get("query", {}).get("search", [])
        if not search_results:
             return {"image_url": None, "summary": None}
             
        page_id = None
        
        if target_name:
            norm_target = target_name.replace(" ", "").replace("ã€€", "")
            for item in search_results:
                title = item["title"].replace(" ", "").replace("ã€€", "")
                if norm_target in title or title in norm_target:
                    page_id = item["pageid"]
                    break
        else:
            page_id = search_results[0]["pageid"]

        if not page_id:
             return {"image_url": None, "summary": None}
        
        info_url = "https://ja.wikipedia.org/w/api.php"
        info_params = {
            "action": "query", 
            "prop": "pageimages|extracts", 
            "pageids": page_id, 
            "pithumbsize": 500,
            "exintro": 1,       
            "explaintext": 1,   
            "exchars": 200,     
            "format": "json"
        }
        info_res = await fetch_with_retry(client, info_url, params=info_params, headers=WIKI_HEADERS, initial_timeout=3.0)
        
        if not info_res: return {"image_url": None, "summary": None}
        
        info_data = info_res.json()
        pages = info_data.get("query", {}).get("pages", {})
        page = pages.get(str(page_id))
        
        if page:
            image_url = page.get("thumbnail", {}).get("source")
            summary = page.get("extract", "").replace("\n", "")
            
            if "å‚ç…§" in summary or "æ›–æ˜§ã•å›é¿" in summary:
                summary = None
            
            if summary and len(summary) >= 200:
                summary = summary.rstrip("ã€ã€‚") + "..."
            
            result = {"image_url": image_url, "summary": summary}
            set_cache(cache_key, result)
            return result
            
    except Exception as e:
        print(f"Wiki info fetch error: {e}")
    
    return {"image_url": None, "summary": None}

async def fetch_spot_coordinates(client, target_name: str, search_query: str):
    cache_key = f"geo_v4:{target_name}:{search_query}"
    cached = get_cache(cache_key)
    if cached: return cached

    try:
        clean_query = re.sub(r'[(ï¼ˆ].*?[)ï¼‰]', '', search_query).strip()
        url = "https://api.geoapify.com/v1/geocode/search"
        params = {"text": clean_query, "apiKey": GEOAPIFY_API_KEY, "lang": "ja", "limit": 3, "countrycode": "jp"}
        res = await fetch_with_retry(client, url, params=params, initial_timeout=8.0, retries=5)

        image_url = None
        wiki_summary = None
        
        if res and res.status_code == 200:
            data = res.json()
            if "features" in data and len(data["features"]) > 0:
                for i, feat in enumerate(data["features"]):
                    props = feat["properties"]
                    result_name = props.get("name", "")
                    if not result_name or not result_name.strip(): continue

                    desc = get_clean_address(props)
                    if not desc: desc = "ä½æ‰€ä¸æ˜"

                    state = props.get("state", "")
                    city = props.get("city", "") or props.get("town", "")
                    
                    wiki_query = f"{result_name} {state}".strip()
                    if len(wiki_query) < len(result_name) + 2:
                            wiki_query = search_query

                    try:
                        wiki_info = await fetch_wikipedia_info(client, wiki_query, target_name=result_name)
                        image_url = wiki_info.get("image_url")
                        if wiki_info.get("summary"):
                            wiki_summary = wiki_info["summary"]
                    except:
                        pass

                    result_data = {
                        "name": result_name, 
                        "description": desc, 
                        "coordinates": feat["geometry"]["coordinates"],
                        "image_url": image_url,
                        "comment": wiki_summary or "" 
                    }
                    set_cache(cache_key, result_data)
                    return result_data
    except Exception as e:
        print(f"Coord fetch failed for {target_name}: {e}")
    
    return None

async def fetch_spot_by_coordinates(client, lat: float, lng: float, fallback_name: str):
    lat_k = round(lat, 6)
    lng_k = round(lng, 6)
    cache_key = f"geo_reverse_v1:{lat_k}:{lng_k}"
    
    cached = get_cache(cache_key)
    if cached: return cached

    try:
        url = "https://api.geoapify.com/v1/geocode/reverse"
        params = {
            "lat": lat, 
            "lon": lng, 
            "apiKey": GEOAPIFY_API_KEY, 
            "lang": "ja", 
            "limit": 1
        }
        
        res = await fetch_with_retry(client, url, params=params, initial_timeout=8.0, retries=3)

        image_url = None
        wiki_summary = None
        
        if res and res.status_code == 200:
            data = res.json()
            if "features" in data and len(data["features"]) > 0:
                props = data["features"][0]["properties"]
                
                result_name = props.get("name", "")
                if not result_name:
                    result_name = fallback_name

                desc = get_clean_address(props)
                desc = re.sub(r'ã€’\d{3}-\d{4}', '', desc).strip()
                if not desc: desc = "ä½æ‰€ä¸æ˜"

                state = props.get("state", "")
                city = props.get("city", "") or props.get("town", "")
                wiki_query = f"{result_name} {state} {city}".strip()

                try:
                    wiki_info = await fetch_wikipedia_info(client, wiki_query, target_name=result_name)
                    image_url = wiki_info.get("image_url")
                    if wiki_info.get("summary"):
                        wiki_summary = wiki_info["summary"]
                except:
                    pass

                result_data = {
                    "name": result_name, 
                    "description": desc,
                    "coordinates": [lng, lat],
                    "image_url": image_url,
                    "comment": wiki_summary or "" 
                }
                set_cache(cache_key, result_data)
                return result_data
    except Exception as e:
        print(f"Reverse Geo Error: {e}")
    
    return None
# main.py ã® get_official_name_by_ai ã®ä»˜è¿‘ã«è¿½åŠ 

async def get_structured_address_by_ai(name: str, raw_address: str = "") -> Dict[str, str]:
    """
    AIã‚’ä½¿ç”¨ã—ã¦ã‚¹ãƒãƒƒãƒˆåã‹ã‚‰æ­£ç¢ºãªä½æ‰€ã‚’ç‰¹å®šã—ã€å¸‚åŒºç”ºæ‘ãƒ¬ãƒ™ãƒ«ã§æ­£è¦åŒ–ã™ã‚‹ã€‚
    """
    cache_key = f"address_fix_v2:{name}:{raw_address}"
    cached = get_cache(cache_key)
    if cached: return cached

    prompt = f"""
    ã‚¿ã‚¹ã‚¯: ã‚¹ãƒãƒƒãƒˆã€Œ{name}ã€ã®æ­£ç¢ºãªä½æ‰€ã‚’ç‰¹å®šã—ã€éƒ½é“åºœçœŒã¨å¸‚åŒºç”ºæ‘ã«åˆ†è§£ã—ã¦ãã ã•ã„ã€‚
    ç¾åœ¨ã®ä¸å®Œå…¨ãªä½æ‰€æƒ…å ±: {raw_address}

    ãƒ«ãƒ¼ãƒ«:
    1. å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã®ã¿ã¨ã™ã‚‹ã“ã¨ã€‚
    {{
      "prefecture": "ã€‡ã€‡çœŒ",
      "city": "ã€‡ã€‡å¸‚",
      "full_address": "ã€‡ã€‡çœŒã€‡ã€‡å¸‚ã€‡ã€‡ç”º1-2-3"
    }}
    2. "city" ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€å¸‚ã€åŒºï¼ˆæ±äº¬23åŒºç­‰ï¼‰ã€ã¾ãŸã¯éƒ¡ã‚’é™¤ã„ãŸç”ºæ‘åã¾ã§ã‚’å…¥ã‚Œã‚‹ã“ã¨ã€‚
    3. å¸‚ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ç”ºåï¼ˆã€‡ã€‡å¸‚â–³â–³ç”ºï¼‰ã®ã€Œâ–³â–³ç”ºã€ã¯å«ã‚ãšã€ã€Œã€‡ã€‡å¸‚ã€ã¨ã™ã‚‹ã“ã¨ã€‚
    4. ä¸æ˜ãªå ´åˆã¯ã€ãã®ã‚¹ãƒãƒƒãƒˆãŒã‚ã‚‹å¯èƒ½æ€§ãŒæœ€ã‚‚é«˜ã„å ´æ‰€ã‚’æ¨è«–ã—ã¦ãã ã•ã„ã€‚
    """

    try:
        res = await aclient.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.0
        )
        data = json.loads(res.choices[0].message.content)
        set_cache(cache_key, data)
        return data
    except Exception as e:
        print(f"AI Address Fix Error: {e}")
        return {"prefecture": "", "city": "", "full_address": raw_address}
# ==========================================
# ğŸ§  AIã«ã‚ˆã‚‹ã‚¯ã‚¨ãƒªæ­£è¦åŒ–é–¢æ•°
# ==========================================
async def get_official_name_by_ai(query: str) -> str:
    """
    AIã‚’ä½¿ç”¨ã—ã¦ã€ã‚ã åãƒ»ã²ã‚‰ãŒãªãƒ»èª¤è¨˜ã‚’ã€Œåœ°å›³æ¤œç´¢ã§ãƒ’ãƒƒãƒˆã—ã‚„ã™ã„æ­£å¼åç§°ã€ã«å¤‰æ›ã™ã‚‹ã€‚
    ä¾‹: "ãã‚‰ã¦ã‚‰ã™" -> "SORA terrace"
    ä¾‹: "ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼" -> "æ±äº¬ãƒ‡ã‚£ã‚ºãƒ‹ãƒ¼ãƒ©ãƒ³ãƒ‰"
    """
    cache_key = f"query_norm_v1:{query}"
    cached = get_cache(cache_key)
    if cached: return cached

    prompt = f"""
    ã‚¿ã‚¹ã‚¯: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ¤œç´¢èªå¥ã€Œ{query}ã€ã‚’ã€Google Mapsã‚„ãƒŠãƒ“ã§æ¤œç´¢ã—ãŸéš›ã«æœ€ã‚‚ãƒ’ãƒƒãƒˆã—ã‚„ã™ã„ã€æ­£å¼åç§°ã€ã¾ãŸã¯ã€æ¼¢å­—è¡¨è¨˜ã€ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
    
    ãƒ«ãƒ¼ãƒ«:
    1. è¦³å…‰åœ°ã‚„æ–½è¨­ã®ã€Œã‚ã åã€ã€Œã²ã‚‰ãŒãªã€ã€Œã‚«ã‚¿ã‚«ãƒŠã€ã€Œãƒ­ãƒ¼ãƒå­—ã€ã¯ã€æ­£å¼ãªè¡¨è¨˜ï¼ˆæ¼¢å­—ã‚„è‹±èªå«ã‚€ï¼‰ã«ç›´ã™ã€‚
    2. æ˜ã‚‰ã‹ãªèª¤å­—è„±å­—ãŒã‚ã‚Œã°ä¿®æ­£ã™ã‚‹ã€‚
    3. ä½æ‰€ã‚„ã€ã™ã§ã«æ­£å¼åç§°ã§ã‚ã‚‹å ´åˆã¯ã€å…¥åŠ›ãã®ã¾ã¾ã‚’è¿”ã™ã€‚
    4. ä½™è¨ˆãªèª¬æ˜ã¯ä¸€åˆ‡ä¸è¦ã€‚ä¿®æ­£å¾Œã®å˜èªã®ã¿ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚

    ä¾‹:
    ãã‚‰ã¦ã‚‰ã™ -> SORA terrace
    äº¬éƒ½ã®ã¿ãšã§ã‚‰ -> æ¸…æ°´å¯º
    ã‚¹ã‚«ã‚¤ãƒ„ãƒªãƒ¼ -> æ±äº¬ã‚¹ã‚«ã‚¤ãƒ„ãƒªãƒ¼
    ãƒ¦ãƒ‹ãƒ -> ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«ãƒ»ã‚¹ã‚¿ã‚¸ã‚ªãƒ»ã‚¸ãƒ£ãƒ‘ãƒ³
    """
    
    try:
        res = await aclient.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=50,
            temperature=0.0
        )
        normalized_name = res.choices[0].message.content.strip()
        normalized_name = normalized_name.replace('"', '').replace("ã€Œ", "").replace("ã€", "")
        
        set_cache(cache_key, normalized_name)
        return normalized_name
    except Exception as e:
        print(f"AI Normalize Error: {e}")
        return query

# ---------------------------------------------------------
# API: å„ç¨®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
# ---------------------------------------------------------

@app.post("/api/nearby_spots")
async def nearby_spots(req: NearbyRequest):
    global http_client
    if http_client is None: return {"spots": []}
    client = http_client

    lat_k = round(req.latitude, 3)
    lon_k = round(req.longitude, 3)
    cache_key = f"nearby_v4:{lat_k}:{lon_k}:{req.radius}:{req.mode}"
    
    cached = get_cache(cache_key)
    if cached: return cached

    try:
        url = "https://api.geoapify.com/v2/places"
        
        if req.mode == "wide":
            categories = "commercial.shopping_mall,commercial.department_store,catering.restaurant,catering.cafe,entertainment,leisure.park,public_transport"
        else:
            categories = "tourism,building.historic,natural,entertainment.culture,religion,natural.cave_entrance"

        params = {
            "categories": categories,
            "filter": f"circle:{req.longitude},{req.latitude},{req.radius}",
            "bias": f"proximity:{req.longitude},{req.latitude}",
            "limit": 20,
            "apiKey": GEOAPIFY_API_KEY,
            "lang": "ja"
        }

        res = await fetch_with_retry(client, url, params=params, initial_timeout=10.0)
        
        base_spots = []
        if res and res.status_code == 200:
            data = res.json()
            if "features" in data:
                for feat in data["features"]:
                    props = feat["properties"]
                    name = props.get("name", "")
                    if not name: continue 

                    geometry = feat.get("geometry")
                    if not geometry or "coordinates" not in geometry: continue
                    coords = geometry["coordinates"]
                    if not isinstance(coords, list) or len(coords) != 2: continue

                    formatted = get_clean_address(props)
                    
                    categories_list = props.get("categories", [])
                    cat_str = "ã‚¹ãƒãƒƒãƒˆ"
                    if "catering" in str(categories_list): cat_str = "ã‚°ãƒ«ãƒ¡"
                    elif "commercial" in str(categories_list): cat_str = "ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°"
                    elif "tourism" in str(categories_list): cat_str = "è¦³å…‰"
                    elif "religion" in str(categories_list): cat_str = "å¯ºç¤¾ä»é–£"
                    elif "natural" in str(categories_list): cat_str = "è‡ªç„¶"

                    state = props.get('state', '')
                    city = props.get('city', '') or props.get('town', '')
                    search_query = f"{name} {state}".strip()

                    base_spots.append({
                        "id": f"nearby-{props.get('place_id')}",
                        "name": name,
                        "description": formatted, 
                        "coordinates": coords,
                        "is_nearby": True,
                        "category": cat_str,
                        "is_commercial": req.mode == "wide",
                        "search_query": search_query,
                        "image_url": None,
                        "comment": "" 
                    })

        async def enrich_spot(spot):
            try:
                wiki_data = await fetch_wikipedia_info(client, spot["search_query"], target_name=spot["name"])
                if wiki_data["image_url"]:
                    spot["image_url"] = wiki_data["image_url"]
                if wiki_data["summary"]:
                    spot["comment"] = wiki_data["summary"] 
            except:
                pass 
            return spot

        if base_spots:
            enriched_spots = await asyncio.gather(*[enrich_spot(s) for s in base_spots])
        else:
            enriched_spots = []

        result = {"spots": enriched_spots}
        if enriched_spots:
            set_cache(cache_key, result)
            
        return result

    except Exception as e:
        print(f"Nearby fetch error: {e}")
        return {"spots": []}


@app.get("/api/get_spot_image")
async def get_spot_image(query: str):
    global http_client
    if http_client is None: return {"image_url": None}
    img_url = await fetch_wikipedia_image(http_client, query)
    return {"image_url": img_url}

@app.post("/api/import_rakuten_hotel")
async def import_rakuten_hotel(req: ImportRequest):
    if not RAKUTEN_APP_ID: return {"error": "ã‚µãƒ¼ãƒãƒ¼è¨­å®šã‚¨ãƒ©ãƒ¼"}
    hotel_no = None
    final_url = req.url
    global http_client
    if http_client is None: return {"error": "Server starting up..."}
    client = http_client

    try:
        cache_key = f"rakuten_import_v2:{req.url}"
        cached = get_cache(cache_key)
        if cached: return cached

        if "rakuten.co.jp" in req.url:
            try:
                res = await fetch_with_retry(client, req.url, initial_timeout=10.0)
                if res: final_url = str(res.url)
            except: pass
        
        match = re.search(r'travel\.rakuten\.co\.jp/.*?/(\d+)', final_url)
        if match: hotel_no = match.group(1)
        else:
            parsed = urllib.parse.urlparse(final_url)
            qs = urllib.parse.parse_qs(parsed.query)
            if "f_no" in qs: hotel_no = qs["f_no"][0]
            elif "no" in qs: hotel_no = qs["no"][0]

        if not hotel_no: return {"error": "URLã‹ã‚‰ãƒ›ãƒ†ãƒ«IDã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"}

        params = {"applicationId": RAKUTEN_APP_ID, "format": "json", "hotelNo": hotel_no, "datumType": 1}
        api_url = "https://app.rakuten.co.jp/services/api/Travel/SimpleHotelSearch/20170426"
        res = await fetch_with_retry(client, api_url, params=params, initial_timeout=15.0)
        
        if not res or res.status_code != 200: return {"error": "ãƒ›ãƒ†ãƒ«æƒ…å ±ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"}

        data = res.json()
        if "hotels" not in data or not data["hotels"]: return {"error": "è©²å½“ã™ã‚‹ãƒ›ãƒ†ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"}

        raw_hotel = data["hotels"][0]
        hotel_content = raw_hotel["hotel"] if "hotel" in raw_hotel else raw_hotel
        
        basic = None
        user_review = {}
        if isinstance(hotel_content, list):
            for item in hotel_content:
                if "hotelBasicInfo" in item: basic = item["hotelBasicInfo"]
                if "userReview" in item: user_review = item["userReview"]
                if "hotelRatingInfo" in item: user_review = item["hotelRatingInfo"]
        else:
            basic = hotel_content.get("hotelBasicInfo")

        if not basic: return {"error": "ãƒ›ãƒ†ãƒ«æƒ…å ±ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸã€‚"}

        address = f"{basic.get('address1', '')}{basic.get('address2', '')}"
        
        spot_data = {
            "id": str(basic["hotelNo"]), "name": basic["hotelName"],
            "description": address, 
            "coordinates": [basic["longitude"], basic["latitude"]],
            "image_url": basic.get("hotelImageUrl"), "url": basic.get("hotelInformationUrl"),
            "price": basic.get("hotelMinCharge", 0), 
            "rating": basic.get("reviewAverage", 3.0),
            
            "service_rating": user_review.get("serviceAverage", 0.0),
            "location_rating": user_review.get("locationAverage", 0.0),
            "room_rating": user_review.get("roomAverage", 0.0),
            "equipment_rating": user_review.get("equipmentAverage", 0.0),
            "bath_rating": user_review.get("bathAverage", 0.0),
            "meal_rating": user_review.get("mealAverage", 0.0),

            "source": "rakuten", "is_hotel": True, "status": "hotel_candidate",
            "comment": basic.get("hotelSpecial", "")[:100] + "..." 
        }
        
        result = {"spot": spot_data}
        set_cache(cache_key, result)
        return result
    except Exception as e:
        print(f"Import Error: {e}")
        return {"error": "å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"}

@app.post("/api/search_hotels_vacant")
async def search_hotels_vacant(req: VacantSearchRequest):
    if not RAKUTEN_APP_ID: return {"error": "ã‚µãƒ¼ãƒãƒ¼è¨­å®šã‚¨ãƒ©ãƒ¼"}
    global http_client
    if http_client is None: return {"error": "Server starting up..."}
    client = http_client

    cache_key = f"rakuten_vacant_v4:{req.latitude}:{req.longitude}:{req.checkin_date}:{req.checkout_date}:{req.adult_num}:{req.min_price}:{req.max_price}:{req.meal_type}:{hashlib.md5(str(req.polygon).encode()).hexdigest() if req.polygon else 'all'}"
    
    cached = get_cache(cache_key)
    if cached: return cached

    safe_radius = min(round(req.radius, 2), 3.0)
    today = date.today()
    c_in = req.checkin_date
    c_out = req.checkout_date
    if not c_in: c_in = (today + timedelta(days=30)).strftime("%Y-%m-%d")
    if not c_out: c_out = (date.fromisoformat(c_in) + timedelta(days=1)).strftime("%Y-%m-%d")

    base_params = {
        "applicationId": RAKUTEN_APP_ID, "format": "json", "latitude": req.latitude, "longitude": req.longitude,
        "searchRadius": safe_radius, "datumType": 1, "hits": 30, "sort": "standard",
        "checkinDate": c_in, "checkoutDate": c_out, "adultNum": req.adult_num,
    }
    if req.max_price: base_params["maxCharge"] = req.max_price
    if req.min_price: base_params["minCharge"] = req.min_price

    if req.meal_type == 'room_only':
        base_params["breakfastFlag"] = 0
        base_params["dinnerFlag"] = 0
    elif req.meal_type == 'breakfast':
        base_params["breakfastFlag"] = 1
        base_params["dinnerFlag"] = 0 
    elif req.meal_type == 'half_board':
        base_params["breakfastFlag"] = 1
        base_params["dinnerFlag"] = 1

    url = "https://app.rakuten.co.jp/services/api/Travel/VacantHotelSearch/20170426"

    async def fetch_page(page_num):
        try:
            p = base_params.copy()
            p["page"] = page_num
            res = await fetch_with_retry(client, url, params=p, initial_timeout=15.0, retries=5)
            if res and res.status_code == 200: return res.json()
        except: pass
        return None

    try:
        results = await asyncio.gather(*[fetch_page(i) for i in range(1, 6)])
        all_hotels = []
        seen_ids = set()

        for data in results:
            if not data or "hotels" not in data: continue
            for h_group in data["hotels"]:
                try:
                    hotel_content = h_group["hotel"] if "hotel" in h_group else h_group
                    if not isinstance(hotel_content, list) or len(hotel_content) == 0: continue
                    
                    basic = None
                    user_review = {}
                    
                    for item in hotel_content:
                        if "hotelBasicInfo" in item:
                            basic = item["hotelBasicInfo"]
                        if "userReview" in item:
                            user_review = item["userReview"]
                        elif "hotelRatingInfo" in item:
                            user_review = item["hotelRatingInfo"]
                            
                    if not basic: continue
                    
                    hotel_id = str(basic["hotelNo"])
                    if hotel_id in seen_ids: continue

                    if req.polygon:
                        if not is_inside_polygon(basic["latitude"], basic["longitude"], req.polygon):
                            continue

                    best_price = float('inf')
                    best_plan_id, best_room_class = None, None
                    found_valid_plan = False
                    
                    for j in range(1, len(hotel_content)):
                        r_info = hotel_content[j].get("roomInfo")
                        if isinstance(r_info, list) and len(r_info) >= 2:
                            r_basic = r_info[0].get("roomBasicInfo", {})
                            
                            if req.meal_type == 'room_only':
                                if r_basic.get("withBreakfastFlag") == 1 or r_basic.get("withDinnerFlag") == 1: continue
                            elif req.meal_type == 'breakfast':
                                if r_basic.get("withBreakfastFlag") != 1: continue
                            elif req.meal_type == 'half_board':
                                if r_basic.get("withBreakfastFlag") != 1 or r_basic.get("withDinnerFlag") != 1: continue

                            r_charge = r_info[1].get("dailyCharge")
                            if r_charge and r_charge.get("total", 0) > 0:
                                if r_charge["total"] < best_price:
                                    best_price = r_charge["total"]
                                    best_plan_id = r_info[0].get("roomBasicInfo", {}).get("planId")
                                    best_room_class = r_info[0].get("roomBasicInfo", {}).get("roomClass")
                                    found_valid_plan = True

                    if not found_valid_plan: continue
                    if req.min_price and best_price < req.min_price: continue
                    if req.max_price and best_price > req.max_price: continue
                    
                    address = f"{basic.get('address1', '')}{basic.get('address2', '')}"
                    
                    all_hotels.append({
                        "id": hotel_id, "name": basic["hotelName"],
                        "description": address, 
                        "coordinates": [basic["longitude"], basic["latitude"]],
                        "image_url": basic.get("hotelImageUrl"), "url": basic.get("hotelInformationUrl"),
                        "price": int(best_price), 
                        
                        "rating": basic.get("reviewAverage") or 0.0,
                        "service_rating": user_review.get("serviceAverage", 0.0),
                        "location_rating": user_review.get("locationAverage", 0.0),
                        "room_rating": user_review.get("roomAverage", 0.0),
                        "equipment_rating": user_review.get("equipmentAverage", 0.0),
                        "bath_rating": user_review.get("bathAverage", 0.0),
                        "meal_rating": user_review.get("mealAverage", 0.0),

                        "review_count": basic.get("reviewCount", 0), "source": "rakuten", "is_hotel": True,
                        "plan_id": best_plan_id, "room_class": best_room_class, "status": "hotel_candidate",
                        "comment": basic.get("hotelSpecial", "")[:60] + "..." 
                    })
                    seen_ids.add(hotel_id)
                except: continue
        
        result = {"hotels": all_hotels}
        if all_hotels: set_cache(cache_key, result)
        return result
    except Exception as e:
        traceback.print_exc()
        return {"error": f"ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼: {str(e)}"}

# ---------------------------------------------------------
# API: AIææ¡ˆ
# ---------------------------------------------------------
async def suggest_spots_generator(req: SuggestRequest):
    global http_client
    if http_client is None:
        yield json.dumps({"type": "error", "message": "Server starting..."}) + "\n"
        return
    client = http_client
    
    yield json.dumps({"type": "status", "message": "AIãŒå€™è£œåœ°ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ä¸­..."}) + "\n"

    existing_names = []
    existing_check_list = []

    for item in req.existing_spots:
        if isinstance(item, dict):
            name = item.get("name", "")
            coords = item.get("coordinates")
            existing_names.append(name)
            existing_check_list.append({"name": name, "coordinates": coords})
        elif isinstance(item, str):
            existing_names.append(item)
            existing_check_list.append({"name": item, "coordinates": None})
        elif hasattr(item, "name"):
            existing_names.append(item.name)
            existing_check_list.append({"name": item.name, "coordinates": item.coordinates})

    prompt = f"""
    å ´æ‰€: {req.theme}
    ã‚¿ã‚¹ã‚¯: è¦³å…‰å®¢ã«äººæ°—ã®ã€Œè¶…æœ‰åãƒ»ç‹é“è¦³å…‰ã‚¹ãƒãƒƒãƒˆã€ã‚’äººæ°—é †ã«15å€‹æŒ™ã’ã¦ãã ã•ã„ã€‚
    æ¡ä»¶:
    1. ãƒ›ãƒ†ãƒ«ã‚„å®¿æ³Šæ–½è¨­ã¯é™¤å¤–ã€‚
    2. æ—¢å­˜ãƒªã‚¹ãƒˆ: {", ".join(existing_names)} ã¯çµ¶å¯¾ã«é™¤å¤–ã€‚
    3. å‡ºåŠ›ã¯JSONå½¢å¼ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆé…åˆ—ã¨ã™ã‚‹ã€‚
    4. å„ã‚¹ãƒãƒƒãƒˆã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®4ã¤ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚ã‚‹ã“ã¨ã€‚
       - "name": åœ°å›³APIã§è¦‹ã¤ã‹ã‚Šã‚„ã™ã„æ­£å¼åç§°
       - "search_query": åœ°å›³æ¤œç´¢ã‚¯ã‚¨ãƒª
       - "summary": ãã®å ´æ‰€ã®é­…åŠ›ã‚„ç‰¹å¾´ã‚’ä¼ãˆã‚‹ã€20ã€œ30æ–‡å­—ç¨‹åº¦ã®é­…åŠ›çš„ãªä¸€è¨€èª¬æ˜æ–‡
       - "category": ãã®å ´æ‰€ã®ã‚«ãƒ†ã‚´ãƒª
    """
    
    target_spots = []
    try:
        ai_res = await aclient.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            max_tokens=1500
        )
        content = ai_res.choices[0].message.content
        json_data = json.loads(content)
        raw_spots = json_data.get("spots", [])
        
        seen_names = set(existing_names)
        for s in raw_spots:
            if s["name"] not in seen_names:
                target_spots.append(s)
                seen_names.add(s["name"])
        target_spots = target_spots[:10]

        yield json.dumps({
            "type": "candidates", 
            "names": [s["name"] for s in target_spots],
            "message": "ä½ç½®æƒ…å ±ã‚’ç…§åˆä¸­..."
        }) + "\n"

    except Exception as e:
        yield json.dumps({"type": "error", "message": f"AIç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}"}) + "\n"
        return

    found_count = 0
    seen_coords = []
    
    async def fetch_and_enrich(spot_info):
        res = await fetch_spot_coordinates(client, spot_info["name"], spot_info["search_query"])
        if res:
            ai_summary = spot_info.get("summary", "")
            if ai_summary:
                res["comment"] = ai_summary
                
            res["category"] = spot_info.get("category", "è¦³å…‰ã‚¹ãƒãƒƒãƒˆ")
            return res
        return None

    tasks = [fetch_and_enrich(s) for s in target_spots]
    
    for future in asyncio.as_completed(tasks):
        try:
            res = await future
            if res and res["coordinates"] != [0.0, 0.0]:
                
                is_duplicate = False
                def normalize(s): return s.replace(" ", "").replace("ã€€", "")
                norm_res_name = normalize(res["name"])
                
                for ex in existing_check_list:
                    if normalize(ex["name"]) == norm_res_name:
                        is_duplicate = True
                        break
                    
                    if ex["coordinates"] and res["coordinates"]:
                        dist = haversine_distance(ex["coordinates"], res["coordinates"])
                        if dist < 0.2: 
                            is_duplicate = True
                            break
                
                if is_duplicate: continue
                if res["coordinates"] in seen_coords: continue

                spot_data = {
                    **res, 
                    "stay_time": 90, 
                    "source": "ai", 
                    "is_hotel": False, 
                    "status": "candidate"
                }
                seen_coords.append(res["coordinates"])
                found_count += 1
                
                yield json.dumps({"type": "spot_found", "spot": spot_data}) + "\n"
        except Exception as e:
            print(f"Error in task: {e}")
            continue
    
    yield json.dumps({"type": "done", "count": found_count}) + "\n"

@app.post("/api/suggest_spots")
async def suggest_spots(req: SuggestRequest):
    return StreamingResponse(suggest_spots_generator(req), media_type="application/x-ndjson")

@app.post("/api/verify_spots")
async def verify_spots(req: VerifyRequest):
    return {"spots": req.spots}

async def calculate_route_fallback(client, ordered_spots, start_min, limit_min):
    if not ordered_spots: return {"error": "ã‚¹ãƒãƒƒãƒˆãŒã‚ã‚Šã¾ã›ã‚“"}
    coords_str = ";".join([f"{s.coordinates[0]:.5f},{s.coordinates[1]:.5f}" for s in ordered_spots[:25]])
    cache_key = f"route:{coords_str}:{start_min}:{limit_min}"
    cached = get_cache(cache_key)
    if cached: return cached

    calc_spots = ordered_spots[:25]
    request_coords = ";".join([f"{s.coordinates[0]},{s.coordinates[1]}" for s in calc_spots])
    url = f"https://api.mapbox.com/directions/v5/mapbox/driving/{request_coords}"
    
    res = await fetch_with_retry(client, url, params={"access_token": MAPBOX_ACCESS_TOKEN, "geometries": "geojson"}, initial_timeout=10.0, retries=5)
    if not res: return {"error": "ãƒ«ãƒ¼ãƒˆè¨ˆç®—APIã¸ã®æ¥ç¶šå¤±æ•—"}
    
    data = res.json()
    if "routes" not in data or not data['routes']: return {"error": "ãƒ«ãƒ¼ãƒˆè¨ˆç®—å¤±æ•—"}
    
    route = data['routes'][0]
    timeline = []
    current = start_min
    
    for i, spot in enumerate(calc_spots):
        stay = spot.stay_time or 60
        arr = current
        dep = arr + stay
        timeline.append({
            "type": "spot", "spot": {**spot.model_dump(), "stay_time": stay},
            "arrival": f"{int(arr//60):02d}:{int(arr%60):02d}",
            "departure": f"{int(dep//60):02d}:{int(dep%60):02d}"
        })
        
        if i < len(calc_spots) - 1:
            dur = 30 
            if i < len(route.get('legs', [])):
                dur = math.ceil(route['legs'][i]['duration'] / 60)
            if i+1 < len(calc_spots):
                gurl = f"http://googleusercontent.com/maps.google.com/?saddr={urllib.parse.quote(spot.name)}&daddr={urllib.parse.quote(calc_spots[i+1].name)}&travelmode=driving"
                timeline.append({"type": "travel", "duration_min": dur, "transport_mode": "car", "google_maps_url": gurl})
            current = dep + dur
            
    used = set(t['spot']['name'] for t in timeline if t['type']=='spot')
    result = {"timeline": timeline, "unused_spots": [s for s in ordered_spots if s.name not in used], "route_geometry": route['geometry']}
    set_cache(cache_key, result)
    return result

@app.post("/api/optimize_route")
@app.post("/api/calculate_route")
async def calculate_route_endpoint(req: OptimizeRequest):
    spots = [s for s in req.spots if s.coordinates and len(s.coordinates) >= 2]
    if len(spots) < 2: return {"error": "2ç®‡æ‰€ä»¥ä¸Šå¿…è¦"}
    global http_client
    if http_client is None: return {"error": "Server starting up..."}
    client = http_client

    try:
        sh, sm = map(int, req.start_time.split(':'))
        eh, em = map(int, req.end_time.split(':'))
        start, limit = sh*60+sm, eh*60+em
    except: start, limit = 540, 1080
    
    return await calculate_route_fallback(client, spots, start, limit)

# ==========================================
# ğŸ” æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ (AIè£œæ­£ç‰ˆ)
# ==========================================
@app.get("/api/search_places")
async def search_places(query: str, lat: Optional[float] = None, lng: Optional[float] = None):
    global http_client
    if http_client is None: return {"results": []}
    client = http_client

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ (çµæœãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ãƒ­ã‚¸ãƒƒã‚¯å¤‰æ›´å¾Œã¯ã‚­ãƒ¼ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å¤‰ãˆã‚‹ã¨è‰¯ã„)
    cache_key_raw = f"search_places_smart_v1:{query}:{lat}:{lng}"
    cached_raw = get_cache(cache_key_raw)
    if cached_raw: return cached_raw

    async def execute_search(search_q):
        """å†…éƒ¨æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ã®å†åˆ©ç”¨ç”¨é–¢æ•°"""
        local_results = []
        seen = set()
        
        # Geocoding API
        try:
            geo_url = "https://api.geoapify.com/v1/geocode/search"
            geo_params = {"text": search_q, "apiKey": GEOAPIFY_API_KEY, "lang": "ja", "limit": 5, "countrycode": "jp"}
            if lat is not None and lng is not None:
                geo_params["bias"] = f"proximity:{lng},{lat}"
            
            res = await fetch_with_retry(client, geo_url, params=geo_params, initial_timeout=5.0)
            if res and res.status_code == 200:
                data = res.json()
                for feat in data.get("features", []):
                    pid = feat["properties"].get("place_id")
                    if pid and pid not in seen:
                        props = feat["properties"]
                        name = props.get("name", "") or props.get("formatted", "").split(",")[0]
                        formatted = props.get("formatted", "")
                        # ä½æ‰€æ•´å½¢
                        clean_fmt = get_clean_address(props)
                        
                        local_results.append({
                            "id": pid, "name": name, "place_name": clean_fmt,
                            "center": feat["geometry"]["coordinates"],
                            "type": "location"
                        })
                        seen.add(pid)
        except Exception: pass
        
        # Places API (è£œå®Œ)
        if len(local_results) < 3:
            try:
                places_url = "https://api.geoapify.com/v2/places"
                p_params = {"name": search_q, "apiKey": GEOAPIFY_API_KEY, "lang": "ja", "limit": 5}
                if lat is not None and lng is not None:
                    p_params["bias"] = f"proximity:{lng},{lat}"
                
                res = await fetch_with_retry(client, places_url, params=p_params, initial_timeout=5.0)
                if res and res.status_code == 200:
                    data = res.json()
                    for feat in data.get("features", []):
                        pid = feat["properties"].get("place_id")
                        if pid and pid not in seen:
                            props = feat["properties"]
                            name = props.get("name", "")
                            if not name: continue
                            clean_fmt = get_clean_address(props)
                            
                            local_results.append({
                                "id": pid, "name": name, "place_name": clean_fmt,
                                "center": feat["geometry"]["coordinates"],
                                "type": "place"
                            })
                            seen.add(pid)
            except Exception: pass
            
        return local_results

    # 1. ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãã®ã¾ã¾ã§æ¤œç´¢
    results = await execute_search(query)

    # 2. çµæœãŒå°‘ãªã„ã€ã¾ãŸã¯ç„¡ã„å ´åˆã€AIè£œæ­£ã‚’ç™ºå‹• (Smart Retry)
    should_ask_ai = False
    if len(results) < 3:
        should_ask_ai = True
    
    if should_ask_ai:
        normalized_query = await get_official_name_by_ai(query)
        
        # AIã®ç­”ãˆãŒå…ƒã®å…¥åŠ›ã¨é•ã†å ´åˆã®ã¿ã€å†æ¤œç´¢ã—ã¦ãƒãƒ¼ã‚¸
        if normalized_query != query:
            ai_results = await execute_search(normalized_query)
            
            existing_ids = {r["id"] for r in results}
            for ar in ai_results:
                if ar["id"] not in existing_ids:
                    results.append(ar)
                    existing_ids.add(ar["id"])

    response_data = {"results": results}
    set_cache(cache_key_raw, response_data)
    return response_data

# main.py ã® @app.get("/api/get_spot_info") ã‚’æ›¸ãæ›ãˆ

# get_spot_info ã‚’ä¿®æ­£
@app.get("/api/get_spot_info")
async def get_spot_info(query: str, lat: Optional[float] = None, lng: Optional[float] = None):
    global http_client
    if http_client is None: return {}
    client = http_client
    
    # æ—¢å­˜ã®æ¤œç´¢å‡¦ç†
    data = None
    if lat is not None and lng is not None:
        data = await fetch_spot_by_coordinates(client, lat, lng, query)
    if not data:
        data = await fetch_spot_coordinates(client, query, query)

    # ä½æ‰€ãŒä¸å®Œå…¨ï¼ˆNN, èª¿æŸ»ä¸­, å¸‚ç”ºæ‘ãŒå«ã¾ã‚Œãªã„ç­‰ï¼‰ãªå ´åˆã«AIã§è£œå®Œ
    current_desc = data.get("description", "") if data else ""
    is_invalid = not current_desc or "NN" in current_desc or "èª¿æŸ»ä¸­" in current_desc or "ä¸æ˜" in current_desc
    
    if is_invalid:
        ai_data = await get_structured_address_by_ai(query, current_desc)
        if ai_data.get("full_address"):
            if not data:
                data = {"name": query, "coordinates": [lng or 0.0, lat or 0.0]}
            data["description"] = ai_data["full_address"]

    # Wikipediaæƒ…å ±ç­‰ã®ãƒãƒ¼ã‚¸
    wiki = await fetch_wikipedia_info(client, query, target_name=query)
    if data:
        data["image_url"] = data.get("image_url") or wiki.get("image_url")
        data["comment"] = data.get("comment") or wiki.get("summary") or ""
        return data
    
    return {"name": query, "description": "", "image_url": wiki.get("image_url"), "comment": wiki.get("summary") or ""}